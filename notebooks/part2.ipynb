{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pdb\n",
    "import six\n",
    "import random\n",
    "import lmdb\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "from itertools import chain\n",
    "import logging\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import sampler\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
    "from torch.nn.utils.clip_grad import clip_grad_norm_\n",
    "from torch.utils.data import random_split\n",
    "sys.path.insert(0, '../')\n",
    "from src.utils.utils import AverageMeter, Eval, OCRLabelConverter\n",
    "from src.utils.utils import EarlyStopping, gmkdir\n",
    "from src.optim.optimizer import STLR\n",
    "from src.utils.utils import gaussian\n",
    "from tqdm import *\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License Plate Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''class PlateDataset(Dataset):\n",
    "    def __init__(self, opt):\n",
    "        super(PlateDataset, self).__init__()\n",
    "        self.path = os.path.join(opt['path'], opt['imgdir'])\n",
    "        self.images = os.listdir(self.path)\n",
    "        self.nSamples = len(self.images)\n",
    "        f = lambda x: os.path.join(self.path, x)\n",
    "        self.imagepaths = list(map(f, self.images))\n",
    "       \ttransform_list =  [transforms.Grayscale(1),\n",
    "                            transforms.ToTensor(), \n",
    "                            transforms.Normalize((0.5,), (0.5,))]\n",
    "        self.transform = transforms.Compose(transform_list)\n",
    "        self.collate_fn = PlateCollator()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nSamples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert index <= len(self), 'index range error'\n",
    "        imagepath = self.imagepaths[index]\n",
    "        imagefile = os.path.basename(imagepath)\n",
    "        img = Image.open(imagepath)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        item = {'img': img, 'idx':index}\n",
    "        item['label'] = imagefile.split('_')[0]\n",
    "        return item \n",
    "    \n",
    "'''\n",
    "\n",
    "class PlateDataset(Dataset):\n",
    "    def __init__(self, opt):\n",
    "        super(PlateDataset, self).__init__()\n",
    "        self.path = os.path.join(opt['path'], opt['imgdir'])\n",
    "        self.annotations_path = os.path.join(opt['path'], 'annotations')  # Assuming annotations are in a folder named 'annotations'\n",
    "        self.images = os.listdir(self.path)\n",
    "        self.nSamples = len(self.images)\n",
    "        f = lambda x: os.path.join(self.path, x)\n",
    "        self.imagepaths = list(map(f, self.images))\n",
    "        # transform_list = [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    "        transform_list = [transforms.ToTensor(),transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])]\n",
    "        self.transform = transforms.Compose(transform_list)\n",
    "        self.collate_fn = PlateCollator()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nSamples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert index <= len(self), 'index range error'\n",
    "        imagepath = self.imagepaths[index]\n",
    "        imagefile = os.path.basename(imagepath)\n",
    "        #print(\"imagefile: \", imagefile)\n",
    "        img = Image.open(imagepath)\n",
    "        #print(\"img.shape: \", img.shape)\n",
    "        # Resize the image to a fixed height\n",
    "        #img = img.resize((180, 32), Image.ANTIALIAS)\n",
    "        \n",
    "        print(\"Min pixel value:\", np.min(img))\n",
    "        print(\"Max pixel value:\", np.max(img))\n",
    "\n",
    "        # Visualize the image\n",
    "        #plt.imshow(img)\n",
    "        #plt.show()\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # Load corresponding annotation\n",
    "        annotation_file = os.path.join(self.annotations_path, imagefile.replace('.png', '.xml'))\n",
    "        labels = self.parse_annotation(annotation_file)\n",
    "\n",
    "        item = {'img': img, 'idx': index, 'label': labels}\n",
    "        return item\n",
    "\n",
    "    def parse_annotation(self, annotation_file):\n",
    "        tree = ET.parse(annotation_file)\n",
    "        root = tree.getroot()\n",
    "        labels = []\n",
    "\n",
    "        for obj in root.findall('object'):\n",
    "            name = obj.find('name').text\n",
    "            labels.append(name)\n",
    "\n",
    "        # Concatenate the values inside <name> tags to form the label\n",
    "        label = ''.join(labels)\n",
    "        #print('label: ', label)\n",
    "\n",
    "        return label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class PlateCollator(object):\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        width = [item['img'].shape[2] for item in batch]\n",
    "        indexes = [item['idx'] for item in batch]\n",
    "        imgs = torch.ones([len(batch), batch[0]['img'].shape[0], batch[0]['img'].shape[1], \n",
    "                           max(width)], dtype=torch.float32)\n",
    "        for idx, item in enumerate(batch):\n",
    "            try:\n",
    "                imgs[idx, :, :, 0:item['img'].shape[2]] = item['img']\n",
    "            except:\n",
    "                print(imgs.shape)\n",
    "        item = {'img': imgs, 'idx':indexes}\n",
    "        if 'label' in batch[0].keys():\n",
    "            labels = [item['label'] for item in batch]\n",
    "            item['label'] = labels\n",
    "        return item\n",
    "    \n",
    "'''\n",
    "\n",
    "\n",
    "class PlateCollator(object):\n",
    "    def __call__(self, batch):\n",
    "        max_height = max(item['img'].shape[1] for item in batch)\n",
    "        max_width = max(item['img'].shape[2] for item in batch)\n",
    "\n",
    "        imgs = torch.ones([len(batch), batch[0]['img'].shape[0], max_height, max_width], dtype=torch.float32)\n",
    "\n",
    "        for idx, item in enumerate(batch):\n",
    "            img = item['img']\n",
    "            print(\"img.shape in collator: \", img.shape)\n",
    "            height, width = img.shape[1], img.shape[2]\n",
    "            imgs[idx, :, :height, :width] = img\n",
    "\n",
    "        indexes = [item['idx'] for item in batch]\n",
    "\n",
    "        item = {'img': imgs, 'idx': indexes, 'label': [item['label'] for item in batch]}\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our Model\n",
    "\n",
    "Now we proceed to define our model. We use the CNN-LSTM based architecture which was proposed by Shi et.al. in their excellent paper [An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition](https://arxiv.org/pdf/1507.05717.pdf). The authors used it for scene-text recognition and showed via extensive experimentation that they were able to achieve significant gains in accuracy compared to all other existing methods at that time.\n",
    "\n",
    "\n",
    "<img src=\"images/crnn.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "The figure above shows the architecture used in the paper. The authors used a 7 layered Convolution network with BatchNorm and ReLU. This was followed by a stacked RNN network consisting of two Bidirectional LSTM layers. The convolution layers acted as a feature extractor while the LSTMs layers act as sequence classifiers. The LSTM layers output the probability associated with each output class at each time step\n",
    "Further details can be found in their paper and I strongly suggest you go through it for a better understanding.\n",
    "\n",
    "The below code snippet is taken from this [github repository](https://github.com/meijieru/crnn.pytorch) which provides a Pytorch implementation of their code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, nIn, nHidden, nOut):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)\n",
    "        self.embedding = nn.Linear(nHidden * 2, nOut)\n",
    "    def forward(self, input):\n",
    "        self.rnn.flatten_parameters()\n",
    "        recurrent, _ = self.rnn(input)\n",
    "        T, b, h = recurrent.size()\n",
    "        t_rec = recurrent.view(T * b, h)\n",
    "        output = self.embedding(t_rec)  # [T * b, nOut]\n",
    "        output = output.view(T, b, -1)\n",
    "        return output\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, opt, leakyRelu=False):\n",
    "        super(CRNN, self).__init__()\n",
    "\n",
    "        assert opt['imgH'] % 16 == 0, 'imgH has to be a multiple of 16'\n",
    "\n",
    "        ks = [3, 3, 3, 3, 3, 3, 2]\n",
    "        ps = [1, 1, 1, 1, 1, 1, 0]\n",
    "        ss = [1, 1, 1, 1, 1, 1, 1]\n",
    "        nm = [64, 128, 256, 256, 512, 512, 512]\n",
    "\n",
    "        cnn = nn.Sequential()\n",
    "\n",
    "        def convRelu(i, batchNormalization=False):\n",
    "            nIn = opt['nChannels'] if i == 0 else nm[i - 1]\n",
    "            nOut = nm[i]\n",
    "            cnn.add_module('conv{0}'.format(i),\n",
    "                           nn.Conv2d(nIn, nOut, ks[i], ss[i], ps[i]))\n",
    "            if batchNormalization:\n",
    "                cnn.add_module('batchnorm{0}'.format(i), nn.BatchNorm2d(nOut))\n",
    "            if leakyRelu:\n",
    "                cnn.add_module('relu{0}'.format(i),\n",
    "                               nn.LeakyReLU(0.2, inplace=True))\n",
    "            else:\n",
    "                cnn.add_module('relu{0}'.format(i), nn.ReLU(True))\n",
    "\n",
    "        convRelu(0)\n",
    "        cnn.add_module('pooling{0}'.format(0), nn.MaxPool2d(2, 2))  # 64x16x64\n",
    "        convRelu(1)\n",
    "        cnn.add_module('pooling{0}'.format(1), nn.MaxPool2d(2, 2))  # 128x8x32\n",
    "        convRelu(2, True)\n",
    "        convRelu(3)\n",
    "        cnn.add_module('pooling{0}'.format(2),\n",
    "                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 256x4x16\n",
    "        convRelu(4, True)\n",
    "        convRelu(5)\n",
    "        cnn.add_module('pooling{0}'.format(3),\n",
    "                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 512x2x16\n",
    "        convRelu(6, True)  # 512x1x16\n",
    "        self.cnn = cnn\n",
    "        self.rnn = nn.Sequential()\n",
    "        self.rnn = nn.Sequential(\n",
    "            BidirectionalLSTM(opt['nHidden']*2, opt['nHidden'], opt['nHidden']),\n",
    "            BidirectionalLSTM(opt['nHidden'], opt['nHidden'], opt['nClasses']))\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        # conv features\n",
    "        conv = self.cnn(input)\n",
    "        b, c, h, w = conv.size()\n",
    "        print(\"Conv Size:\", conv.size())\n",
    "\n",
    "        if h != 1:\n",
    "            # Perform resizing or cropping to make height 1\n",
    "            # Example: \n",
    "            conv = F.adaptive_avg_pool2d(conv, (1, w))\n",
    "            print(\"Resized/Cropped Conv Size:\", conv.size())\n",
    "\n",
    "        # assert h == 1, \"the height of conv must be 1\"\n",
    "        conv = conv.squeeze(2)\n",
    "        conv = conv.permute(2, 0, 1)  # [w, b, c]\n",
    "\n",
    "        # rnn features\n",
    "        output = self.rnn(conv)\n",
    "        output = output.transpose(1, 0)  # Tbh to bth\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCTCLoss(torch.nn.Module):\n",
    "    # T x B x H => Softmax on dimension 2\n",
    "    def __init__(self, dim=2):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.ctc_loss = torch.nn.CTCLoss(reduction='mean', zero_infinity=True)\n",
    "\n",
    "    def forward(self, logits, labels,\n",
    "            prediction_sizes, target_sizes):\n",
    "        EPS = 1e-7\n",
    "        loss = self.ctc_loss(logits, labels, prediction_sizes, target_sizes)\n",
    "        loss = self.sanitize(loss)\n",
    "        return self.debug(loss, logits, labels, prediction_sizes, target_sizes)\n",
    "    \n",
    "    def sanitize(self, loss):\n",
    "        EPS = 1e-7\n",
    "        if abs(loss.item() - float('inf')) < EPS:\n",
    "            return torch.zeros_like(loss)\n",
    "        if math.isnan(loss.item()):\n",
    "            return torch.zeros_like(loss)\n",
    "        return loss\n",
    "\n",
    "    def debug(self, loss, logits, labels,\n",
    "            prediction_sizes, target_sizes):\n",
    "        if math.isnan(loss.item()):\n",
    "            print(\"Loss:\", loss)\n",
    "            print(\"logits:\", logits)\n",
    "            print(\"labels:\", labels)\n",
    "            print(\"prediction_sizes:\", prediction_sizes)\n",
    "            print(\"target_sizes:\", target_sizes)\n",
    "            raise Exception(\"NaN loss obtained. But why?\")\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Training Loop\n",
    "\n",
    "The above code snippet builds a wrapper around pytorch's CTC loss function. Basically, what it does is that it computes the loss and passes it through an additional method called `debug`, which checks for instances when the loss becomes Nan. \n",
    "\n",
    "Shout out to [Jerin Philip](https://jerinphilip.github.io/) for this code.\n",
    "\n",
    "Now, let us come to the training loop. The below code might look a bit cumbersome but it provides a nice abstraction which is quite intuitive and easy to use. The below code is based on [pytorch lighning](https://github.com/PyTorchLightning/pytorch-lightning)'s bolier plate template with few modifications of my own. :P\n",
    "\n",
    "I will give a basic overview of what it does. Feel free to inspect each method using python debugger. So, the `OCRTrainer` class takes in the training and validation data. It also takes in the loss function, optimizer and the number of epoch it needs to train the model. The train and validation loader method returns the data loader for the train and validation data. the `run_batch` method does one forward pass for a batch of image-label pairs. It returns the loss as well as the character and word accuracy. Next, we have the step functions which does the backpropagation, calculates the gradient and updates the parameters for each batch of data. Besides we also have the `training_end` and `validation_end` methods that calculate the mean loss and accuracy for each batch after the completion of one single epoch\n",
    "\n",
    "All, the methods defined are quite simple and I hope you will get the hang of it in no time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCRTrainer(object):\n",
    "    def __init__(self, opt):\n",
    "        super(OCRTrainer, self).__init__()\n",
    "        self.data_train = opt['data_train']\n",
    "        self.data_val = opt['data_val']\n",
    "        self.model = opt['model']\n",
    "        self.criterion = opt['criterion']\n",
    "        self.optimizer = opt['optimizer']\n",
    "        self.schedule = opt['schedule']\n",
    "        self.converter = OCRLabelConverter(opt['alphabet'])\n",
    "        self.evaluator = Eval()\n",
    "        print('Scheduling is {}'.format(self.schedule))\n",
    "        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=opt['epochs'])\n",
    "        self.batch_size = opt['batch_size']\n",
    "        self.count = opt['epoch']\n",
    "        self.epochs = opt['epochs']\n",
    "        self.cuda = opt['cuda']\n",
    "        self.collate_fn = opt['collate_fn']\n",
    "        self.init_meters()\n",
    "\n",
    "    def init_meters(self):\n",
    "        self.avgTrainLoss = AverageMeter(\"Train loss\")\n",
    "        self.avgTrainCharAccuracy = AverageMeter(\"Train Character Accuracy\")\n",
    "        self.avgTrainWordAccuracy = AverageMeter(\"Train Word Accuracy\")\n",
    "        self.avgValLoss = AverageMeter(\"Validation loss\")\n",
    "        self.avgValCharAccuracy = AverageMeter(\"Validation Character Accuracy\")\n",
    "        self.avgValWordAccuracy = AverageMeter(\"Validation Word Accuracy\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.model(x)\n",
    "        return logits.transpose(1, 0)\n",
    "\n",
    "    def loss_fn(self, logits, targets, pred_sizes, target_sizes):\n",
    "        loss = self.criterion(logits, targets, pred_sizes, target_sizes)\n",
    "        return loss\n",
    "\n",
    "    def step(self):\n",
    "        self.max_grad_norm = 0.05\n",
    "        clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def schedule_lr(self):\n",
    "        if self.schedule:\n",
    "            self.scheduler.step()\n",
    "\n",
    "    def _run_batch(self, batch, report_accuracy=False, validation=False):\n",
    "        input_, targets = batch['img'], batch['label']\n",
    "        targets, lengths = self.converter.encode(targets)\n",
    "        logits = self.forward(input_)\n",
    "        logits = logits.contiguous().cpu()\n",
    "        logits = torch.nn.functional.log_softmax(logits, 2)\n",
    "        T, B, H = logits.size()\n",
    "        pred_sizes = torch.LongTensor([T for i in range(B)])\n",
    "        targets= targets.view(-1).contiguous()\n",
    "        loss = self.loss_fn(logits, targets, pred_sizes, lengths)\n",
    "        if report_accuracy:\n",
    "            probs, preds = logits.max(2)\n",
    "            preds = preds.transpose(1, 0).contiguous().view(-1)\n",
    "            sim_preds = self.converter.decode(preds.data, pred_sizes.data, raw=False)\n",
    "            ca = np.mean((list(map(self.evaluator.char_accuracy, list(zip(sim_preds, batch['label']))))))\n",
    "            wa = np.mean((list(map(self.evaluator.word_accuracy, list(zip(sim_preds, batch['label']))))))\n",
    "        return loss, ca, wa\n",
    "\n",
    "    def run_epoch(self, validation=False):\n",
    "        if not validation:\n",
    "            loader = self.train_dataloader()\n",
    "            pbar = tqdm(loader, desc='Epoch: [%d]/[%d] Training'%(self.count, \n",
    "                self.epochs), leave=True)\n",
    "            self.model.train()\n",
    "        else:\n",
    "            loader = self.val_dataloader()\n",
    "            pbar = tqdm(loader, desc='Validating', leave=True)\n",
    "            self.model.eval()\n",
    "        outputs = []\n",
    "        for batch_nb, batch in enumerate(pbar):\n",
    "            if not validation:\n",
    "                output = self.training_step(batch)\n",
    "            else:\n",
    "                output = self.validation_step(batch)\n",
    "            pbar.set_postfix(output)\n",
    "            outputs.append(output)\n",
    "        self.schedule_lr()\n",
    "        if not validation:\n",
    "            result = self.train_end(outputs)\n",
    "        else:\n",
    "            result = self.validation_end(outputs)\n",
    "        return result\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        loss, ca, wa = self._run_batch(batch, report_accuracy=True)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.step()\n",
    "        output = OrderedDict({\n",
    "            'loss': abs(loss.item()),\n",
    "            'train_ca': ca.item(),\n",
    "            'train_wa': wa.item()\n",
    "            })\n",
    "        return output\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        loss, ca, wa = self._run_batch(batch, report_accuracy=True, validation=True)\n",
    "        output = OrderedDict({\n",
    "            'val_loss': abs(loss.item()),\n",
    "            'val_ca': ca.item(),\n",
    "            'val_wa': wa.item()\n",
    "            })\n",
    "        return output\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        # logging.info('training data loader called')\n",
    "        loader = torch.utils.data.DataLoader(self.data_train,\n",
    "                batch_size=self.batch_size,\n",
    "                collate_fn=self.collate_fn,\n",
    "                shuffle=True)\n",
    "        return loader\n",
    "        \n",
    "    def val_dataloader(self):\n",
    "        # logging.info('val data loader called')\n",
    "        loader = torch.utils.data.DataLoader(self.data_val,\n",
    "                batch_size=self.batch_size,\n",
    "                collate_fn=self.collate_fn)\n",
    "        return loader\n",
    "\n",
    "    def train_end(self, outputs):\n",
    "        for output in outputs:\n",
    "            self.avgTrainLoss.add(output['loss'])\n",
    "            self.avgTrainCharAccuracy.add(output['train_ca'])\n",
    "            self.avgTrainWordAccuracy.add(output['train_wa'])\n",
    "\n",
    "        train_loss_mean = abs(self.avgTrainLoss.compute())\n",
    "        train_ca_mean = self.avgTrainCharAccuracy.compute()\n",
    "        train_wa_mean = self.avgTrainWordAccuracy.compute()\n",
    "\n",
    "        result = {'train_loss': train_loss_mean, 'train_ca': train_ca_mean,\n",
    "        'train_wa': train_wa_mean}\n",
    "        # result = {'progress_bar': tqdm_dict, 'log': tqdm_dict, 'val_loss': train_loss_mean}\n",
    "        return result\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        for output in outputs:\n",
    "            self.avgValLoss.add(output['val_loss'])\n",
    "            self.avgValCharAccuracy.add(output['val_ca'])\n",
    "            self.avgValWordAccuracy.add(output['val_wa'])\n",
    "\n",
    "        val_loss_mean = abs(self.avgValLoss.compute())\n",
    "        val_ca_mean = self.avgValCharAccuracy.compute()\n",
    "        val_wa_mean = self.avgValWordAccuracy.compute()\n",
    "\n",
    "        result = {'val_loss': val_loss_mean, 'val_ca': val_ca_mean,\n",
    "        'val_wa': val_wa_mean}\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting Everything Together\n",
    "\n",
    "And, finally, we have the `Learner` class. It implements a couple of more methods like the `save` and `load` model. It also tracks the losses and saves them in a `csv` file. This comes in handy if we want to analyze the behaviour of our training and validation loops. It initializes our `OCRTrainer` module with the necessary hyperparameters and later calls the `fit` method which runs the training loop.\n",
    "\n",
    "Besides these methods, we have a bunch of helper methods like the `OCRLabel_converter`, `Eval` and `Averagemeter`. I am not including them in this notebook, instead, I have written them in utils.py file and I am importing them from there. In case you want to take a peek, feel free to tinker with the utils.py file. All the necessary documentation is provided in the file itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "available devices: 4\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "print(f'available devices: {torch.cuda.device_count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner(object):\n",
    "    def __init__(self, model, optimizer, savepath=None, resume=False):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.savepath = os.path.join(savepath, 'best.ckpt')\n",
    "        self.cuda = torch.cuda.is_available() \n",
    "        self.cuda_count = torch.cuda.device_count()\n",
    "        if self.cuda:\n",
    "            self.model = self.model.cuda()\n",
    "        self.epoch = 0\n",
    "        if self.cuda_count > 1:\n",
    "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "        self.best_score = None\n",
    "        if resume and os.path.exists(self.savepath):\n",
    "            self.checkpoint = torch.load(self.savepath)\n",
    "            self.epoch = self.checkpoint['epoch']\n",
    "            self.best_score=self.checkpoint['best']\n",
    "            self.load()\n",
    "        else:\n",
    "            print('checkpoint does not exist')\n",
    "\n",
    "    def fit(self, opt):\n",
    "        print(\"running fit method\")\n",
    "        opt['cuda'] = self.cuda\n",
    "        opt['model'] = self.model\n",
    "        opt['optimizer'] = self.optimizer\n",
    "        logging.basicConfig(filename=\"%s/%s.csv\" %(opt['log_dir'], opt['name']), level=logging.INFO)\n",
    "        self.saver = EarlyStopping(self.savepath, patience=15, verbose=True, best_score=self.best_score)\n",
    "        opt['epoch'] = self.epoch\n",
    "        trainer = OCRTrainer(opt)\n",
    "        \n",
    "        for epoch in range(opt['epoch'], opt['epochs']):\n",
    "            train_result = trainer.run_epoch()\n",
    "            val_result = trainer.run_epoch(validation=True)\n",
    "            trainer.count = epoch\n",
    "            info = '%d, %.6f, %.6f, %.6f, %.6f, %.6f, %.6f'%(epoch, train_result['train_loss'], \n",
    "                val_result['val_loss'], train_result['train_ca'],  val_result['val_ca'],\n",
    "                train_result['train_wa'], val_result['val_wa'])\n",
    "            logging.info(info)\n",
    "            self.val_loss = val_result['val_loss']\n",
    "            print(self.val_loss)\n",
    "            if self.savepath:\n",
    "                self.save(epoch)\n",
    "            if self.saver.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    def load(self):\n",
    "        print('Loading checkpoint at {} trained for {} epochs'.format(self.savepath, self.checkpoint['epoch']))\n",
    "        self.model.load_state_dict(self.checkpoint['state_dict'], strict=False)\n",
    "        if 'opt_state_dict' in self.checkpoint.keys():\n",
    "            print('Loading optimizer')\n",
    "            self.optimizer.load_state_dict(self.checkpoint['opt_state_dict'], strict=False)\n",
    "\n",
    "    def save(self, epoch):\n",
    "        self.saver(self.val_loss, epoch, self.model, self.optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the hyperparameters\n",
    "\n",
    "Okay, now that we have set the premise, its time to unfold the drama. We begin by defining our vocabulary i.e. the alphabets which serve as the output classes for our model.\n",
    "We define a suitable name for this experiment which will also serve as the folder name where the checkpoints and log files will be stored. We also define the hyper-parameters like the batch size, learning rate, image height, number of channels etc.\n",
    "\n",
    "Then we initialize our Dataset class and split the data into train and validation. We then proceed to initialize our Model and CTCLoss and finally call the `learner.fit` function.\n",
    "\n",
    "Once the training is over we can find the saved model in the `checkpoints/name` folder. We may load the model and evaluate its performance on the test data or finetune it on some other data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traininig Data Size:133\n",
      "Val Data Size:34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: [0]/[4] Training:   0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 4 GPUs!\n",
      "checkpoint does not exist\n",
      "running fit method\n",
      "None\n",
      "Scheduling is False\n",
      "img.shape in collator:  torch.Size([3, 97, 207])\n",
      "img.shape in collator:  torch.Size([3, 56, 144])\n",
      "img.shape in collator:  torch.Size([3, 236, 489])\n",
      "img.shape in collator:  torch.Size([3, 177, 747])\n",
      "img.shape in collator:  torch.Size([3, 52, 210])\n",
      "img.shape in collator:  torch.Size([3, 59, 194])\n",
      "img.shape in collator:  torch.Size([3, 48, 142])\n",
      "img.shape in collator:  torch.Size([3, 97, 276])\n",
      "img.shape in collator:  torch.Size([3, 91, 295])\n",
      "img.shape in collator:  torch.Size([3, 52, 168])\n",
      "img.shape in collator:  torch.Size([3, 62, 210])\n",
      "img.shape in collator:  torch.Size([3, 80, 268])\n",
      "Conv Size: torch.Size([3, 512, 13, 187])\n",
      "Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 13, 187])\n",
      "Conv Size:Conv Size: torch.Size([3, 512, 1, 187])\n",
      " torch.Size([3, 512, 13, 187])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 13, 187])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 187])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 187])\n",
      " torch.Size([3, 512, 1, 187])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[4] Training:  17%|█▋        | 2/12 [00:00<00:03,  2.69it/s, loss=25.6, train_ca=0, train_wa=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape in collator:  torch.Size([3, 75, 215])\n",
      "img.shape in collator:  torch.Size([3, 61, 97])\n",
      "img.shape in collator:  torch.Size([3, 208, 283])\n",
      "img.shape in collator:  torch.Size([3, 63, 223])\n",
      "img.shape in collator:  torch.Size([3, 75, 281])\n",
      "img.shape in collator:  torch.Size([3, 68, 216])\n",
      "img.shape in collator:  torch.Size([3, 64, 202])\n",
      "img.shape in collator:  torch.Size([3, 60, 110])\n",
      "img.shape in collator:  torch.Size([3, 66, 205])\n",
      "img.shape in collator:  torch.Size([3, 83, 225])\n",
      "img.shape in collator:  torch.Size([3, 74, 173])\n",
      "img.shape in collator:  torch.Size([3, 69, 212])\n",
      "Conv Size: torch.Size([3, 512, 12, 71])\n",
      "Resized/Cropped Conv Size:Conv Size:Conv Size: torch.Size([3, 512, 12, 71])\n",
      "Conv Size: torch.Size([3, 512, 1, 71])\n",
      " torch.Size([3, 512, 12, 71])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 12, 71])\n",
      " torch.Size([3, 512, 1, 71])\n",
      "Resized/Cropped Conv Size:Resized/Cropped Conv Size:  torch.Size([3, 512, 1, 71])\n",
      "torch.Size([3, 512, 1, 71])\n",
      "img.shape in collator:  torch.Size([3, 62, 137])\n",
      "img.shape in collator:  torch.Size([3, 43, 149])\n",
      "img.shape in collator:  torch.Size([3, 72, 198])\n",
      "img.shape in collator:  torch.Size([3, 74, 217])\n",
      "img.shape in collator:  torch.Size([3, 62, 249])\n",
      "img.shape in collator:  torch.Size([3, 86, 291])\n",
      "img.shape in collator:  torch.Size([3, 76, 169])\n",
      "img.shape in collator:  torch.Size([3, 491, 2126])\n",
      "img.shape in collator:  torch.Size([3, 36, 72])\n",
      "img.shape in collator:  torch.Size([3, 74, 133])\n",
      "img.shape in collator:  torch.Size([3, 388, 1301])\n",
      "img.shape in collator:  torch.Size([3, 47, 119])\n",
      "Conv Size: torch.Size([3, 512, 29, 532])\n",
      "Resized/Cropped Conv Size:Conv Size:Conv Size: torch.Size([3, 512, 29, 532])\n",
      " torch.Size([3, 512, 1, 532])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 532])\n",
      "Conv Size: torch.Size([3, 512, 29, 532])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 29, 532])\n",
      " torch.Size([3, 512, 1, 532])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 532])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[4] Training:  33%|███▎      | 4/12 [00:01<00:03,  2.58it/s, loss=13.4, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape in collator:  torch.Size([3, 62, 168])\n",
      "img.shape in collator:  torch.Size([3, 56, 165])\n",
      "img.shape in collator:  torch.Size([3, 52, 145])\n",
      "img.shape in collator:  torch.Size([3, 107, 310])\n",
      "img.shape in collator:  torch.Size([3, 312, 477])\n",
      "img.shape in collator:  torch.Size([3, 74, 161])\n",
      "img.shape in collator:  torch.Size([3, 133, 237])\n",
      "img.shape in collator:  torch.Size([3, 45, 128])\n",
      "img.shape in collator:  torch.Size([3, 71, 183])\n",
      "img.shape in collator:  torch.Size([3, 55, 124])\n",
      "img.shape in collator:  torch.Size([3, 154, 416])\n",
      "img.shape in collator:  torch.Size([3, 74, 205])\n",
      "Conv Size: torch.Size([3, 512, 18, 120])\n",
      "Resized/Cropped Conv Size:Conv Size: torch.Size([3, 512, 1, 120])\n",
      " torch.Size([3, 512, 18, 120])\n",
      "Resized/Cropped Conv Size:Conv Size: torch.Size([3, 512, 1, 120])\n",
      "Conv Size:  torch.Size([3, 512, 18, 120])\n",
      "Resized/Cropped Conv Size:torch.Size([3, 512, 18, 120])\n",
      " torch.Size([3, 512, 1, 120])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 120])\n",
      "img.shape in collator:  torch.Size([3, 104, 370])\n",
      "img.shape in collator:  torch.Size([3, 199, 408])\n",
      "img.shape in collator:  torch.Size([3, 39, 118])\n",
      "img.shape in collator:  torch.Size([3, 94, 280])\n",
      "img.shape in collator:  torch.Size([3, 77, 239])\n",
      "img.shape in collator:  torch.Size([3, 147, 690])\n",
      "img.shape in collator:  torch.Size([3, 64, 183])\n",
      "img.shape in collator:  torch.Size([3, 93, 402])\n",
      "img.shape in collator:  torch.Size([3, 48, 134])\n",
      "img.shape in collator:  torch.Size([3, 48, 67])\n",
      "img.shape in collator:  torch.Size([3, 152, 269])\n",
      "img.shape in collator:  torch.Size([3, 89, 210])\n",
      "Conv Size:Conv Size: torch.Size([3, 512, 11, 173])\n",
      " torch.Size([3, 512, 11, 173])\n",
      "Conv Size: torch.Size([3, 512, 11, 173])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 173])\n",
      "Conv Size: torch.Size([3, 512, 11, 173])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 173])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 173])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 173])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[4] Training:  42%|████▏     | 5/12 [00:01<00:02,  3.07it/s, loss=5.46, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape in collator:  torch.Size([3, 67, 291])\n",
      "img.shape in collator:  torch.Size([3, 52, 168])\n",
      "img.shape in collator:  torch.Size([3, 72, 152])\n",
      "img.shape in collator:  torch.Size([3, 43, 74])\n",
      "img.shape in collator:  torch.Size([3, 109, 199])\n",
      "img.shape in collator:  torch.Size([3, 50, 118])\n",
      "img.shape in collator:  torch.Size([3, 526, 2043])\n",
      "img.shape in collator:  torch.Size([3, 60, 159])\n",
      "img.shape in collator:  torch.Size([3, 56, 113])\n",
      "img.shape in collator:  torch.Size([3, 59, 129])\n",
      "img.shape in collator:  torch.Size([3, 163, 464])\n",
      "img.shape in collator:  torch.Size([3, 48, 209])\n",
      "Conv Size:Conv Size: torch.Size([3, 512, 31, 511])\n",
      "Conv Size: torch.Size([3, 512, 31, 511])\n",
      "Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 31, 511])\n",
      " torch.Size([3, 512, 1, 511])Resized/Cropped Conv Size: torch.Size([3, 512, 1, 511])\n",
      " torch.Size([3, 512, 31, 511])\n",
      "Resized/Cropped Conv Size:\n",
      "Resized/Cropped Conv Size:  torch.Size([3, 512, 1, 511])\n",
      "torch.Size([3, 512, 1, 511])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[4] Training:  58%|█████▊    | 7/12 [00:02<00:01,  2.85it/s, loss=6.09, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape in collator:  torch.Size([3, 39, 179])\n",
      "img.shape in collator:  torch.Size([3, 37, 189])\n",
      "img.shape in collator:  torch.Size([3, 129, 312])\n",
      "img.shape in collator:  torch.Size([3, 66, 176])\n",
      "img.shape in collator:  torch.Size([3, 46, 105])\n",
      "img.shape in collator:  torch.Size([3, 75, 300])\n",
      "img.shape in collator:  torch.Size([3, 47, 124])\n",
      "img.shape in collator:  torch.Size([3, 58, 176])\n",
      "img.shape in collator:  torch.Size([3, 56, 126])\n",
      "img.shape in collator:  torch.Size([3, 97, 459])\n",
      "img.shape in collator:  torch.Size([3, 110, 399])\n",
      "img.shape in collator:  torch.Size([3, 59, 151])\n",
      "Conv Size: torch.Size([3, 512, 7, 115])\n",
      "Conv Size:Conv Size: torch.Size([3, 512, 7, 115])\n",
      " torch.Size([3, 512, 7, 115])Resized/Cropped Conv Size:\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 115])\n",
      " torch.Size([3, 512, 1, 115])\n",
      "Conv Size: torch.Size([3, 512, 7, 115])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 115])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 115])\n",
      "img.shape in collator:  torch.Size([3, 93, 328])\n",
      "img.shape in collator:  torch.Size([3, 53, 221])\n",
      "img.shape in collator:  torch.Size([3, 54, 148])\n",
      "img.shape in collator:  torch.Size([3, 57, 177])\n",
      "img.shape in collator:  torch.Size([3, 91, 214])\n",
      "img.shape in collator:  torch.Size([3, 66, 161])\n",
      "img.shape in collator:  torch.Size([3, 62, 181])\n",
      "img.shape in collator:  torch.Size([3, 35, 110])\n",
      "img.shape in collator:  torch.Size([3, 334, 657])\n",
      "img.shape in collator:  torch.Size([3, 301, 528])\n",
      "img.shape in collator:  torch.Size([3, 49, 120])\n",
      "img.shape in collator:  torch.Size([3, 115, 209])\n",
      "Conv Size: torch.Size([3, 512, 19, 165])\n",
      "Conv Size: torch.Size([3, 512, 19, 165])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 165])\n",
      "Resized/Cropped Conv Size:Conv Size:Conv Size: torch.Size([3, 512, 19, 165])\n",
      "  torch.Size([3, 512, 19, 165])\n",
      "Resized/Cropped Conv Size:torch.Size([3, 512, 1, 165])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 165])\n",
      " torch.Size([3, 512, 1, 165])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[4] Training:  75%|███████▌  | 9/12 [00:02<00:00,  3.91it/s, loss=6.7, train_ca=0, train_wa=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape in collator:  torch.Size([3, 46, 124])\n",
      "img.shape in collator:  torch.Size([3, 52, 159])\n",
      "img.shape in collator:  torch.Size([3, 63, 238])\n",
      "img.shape in collator:  torch.Size([3, 110, 403])\n",
      "img.shape in collator:  torch.Size([3, 73, 283])\n",
      "img.shape in collator:  torch.Size([3, 59, 230])\n",
      "img.shape in collator:  torch.Size([3, 83, 209])\n",
      "img.shape in collator:  torch.Size([3, 47, 139])\n",
      "img.shape in collator:  torch.Size([3, 73, 203])\n",
      "img.shape in collator:  torch.Size([3, 154, 406])\n",
      "img.shape in collator:  torch.Size([3, 114, 144])\n",
      "img.shape in collator:  torch.Size([3, 133, 272])\n",
      "Conv Size:Conv Size: torch.Size([3, 512, 8, 102])\n",
      "Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 1, 102])\n",
      "Conv Size: torch.Size([3, 512, 8, 102])\n",
      " torch.Size([3, 512, 8, 102])\n",
      "Resized/Cropped Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 1, 102])\n",
      " torch.Size([3, 512, 8, 102])\n",
      " torch.Size([3, 512, 1, 102])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 102])\n",
      "img.shape in collator:  torch.Size([3, 52, 125])\n",
      "img.shape in collator:  torch.Size([3, 62, 255])\n",
      "img.shape in collator:  torch.Size([3, 509, 1158])\n",
      "img.shape in collator:  torch.Size([3, 57, 188])\n",
      "img.shape in collator:  torch.Size([3, 64, 127])\n",
      "img.shape in collator:  torch.Size([3, 52, 139])\n",
      "img.shape in collator:  torch.Size([3, 132, 373])\n",
      "img.shape in collator:  torch.Size([3, 98, 206])\n",
      "img.shape in collator:  torch.Size([3, 59, 80])\n",
      "img.shape in collator:  torch.Size([3, 56, 155])\n",
      "img.shape in collator:  torch.Size([3, 69, 134])\n",
      "img.shape in collator:  torch.Size([3, 239, 813])\n",
      "Conv Size:Conv Size: torch.Size([3, 512, 30, 290])\n",
      "Resized/Cropped Conv Size: Conv Size: torch.Size([3, 512, 30, 290])\n",
      " torch.Size([3, 512, 1, 290])\n",
      "Resized/Cropped Conv Size:Conv Size:torch.Size([3, 512, 30, 290])\n",
      " torch.Size([3, 512, 30, 290])Resized/Cropped Conv Size: torch.Size([3, 512, 1, 290])\n",
      " torch.Size([3, 512, 1, 290])\n",
      "\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 290])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[4] Training:  83%|████████▎ | 10/12 [00:03<00:00,  3.27it/s, loss=5.23, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape in collator:  torch.Size([3, 49, 162])\n",
      "img.shape in collator:  torch.Size([3, 78, 196])\n",
      "img.shape in collator:  torch.Size([3, 74, 157])\n",
      "img.shape in collator:  torch.Size([3, 77, 157])\n",
      "img.shape in collator:  torch.Size([3, 51, 111])\n",
      "img.shape in collator:  torch.Size([3, 61, 198])\n",
      "img.shape in collator:  torch.Size([3, 62, 201])\n",
      "img.shape in collator:  torch.Size([3, 89, 313])\n",
      "img.shape in collator:  torch.Size([3, 110, 153])\n",
      "img.shape in collator:  torch.Size([3, 56, 143])\n",
      "img.shape in collator:  torch.Size([3, 127, 479])\n",
      "img.shape in collator:  torch.Size([3, 294, 1374])\n",
      "Conv Size:Conv Size:  torch.Size([3, 512, 17, 344])\n",
      "Resized/Cropped Conv Size:Conv Size:torch.Size([3, 512, 17, 344])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 17, 344])\n",
      "Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 1, 344])\n",
      "  torch.Size([3, 512, 1, 344])torch.Size([3, 512, 1, 344])\n",
      "\n",
      " torch.Size([3, 512, 17, 344])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 344])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[4] Training: 100%|██████████| 12/12 [00:03<00:00,  3.25it/s, loss=5.22, train_ca=nan, train_wa=nan]\n",
      "Validating:  33%|███▎      | 1/3 [00:00<00:00,  9.61it/s, val_loss=3.93, val_ca=0, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape in collator:  torch.Size([3, 49, 180])\n",
      "Conv Size: torch.Size([1, 512, 2, 46])\n",
      "Resized/Cropped Conv Size: torch.Size([1, 512, 1, 46])\n",
      "img.shape in collator:  torch.Size([3, 167, 247])\n",
      "img.shape in collator:  torch.Size([3, 293, 684])\n",
      "img.shape in collator:  torch.Size([3, 44, 110])\n",
      "img.shape in collator:  torch.Size([3, 47, 144])\n",
      "img.shape in collator:  torch.Size([3, 59, 177])\n",
      "img.shape in collator:  torch.Size([3, 114, 550])\n",
      "img.shape in collator:  torch.Size([3, 108, 351])\n",
      "img.shape in collator:  torch.Size([3, 68, 164])\n",
      "img.shape in collator:  torch.Size([3, 110, 203])\n",
      "img.shape in collator:  torch.Size([3, 33, 97])\n",
      "img.shape in collator:  torch.Size([3, 68, 209])\n",
      "img.shape in collator:  torch.Size([3, 87, 243])\n",
      "Conv Size: torch.Size([3, 512, 17, 172])\n",
      "Conv Size: torch.Size([3, 512, 17, 172])\n",
      "Resized/Cropped Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 1, 172])\n",
      "Conv Size:Conv Size: torch.Size([3, 512, 1, 172]) torch.Size([3, 512, 17, 172])\n",
      " torch.Size([3, 512, 17, 172])\n",
      "\n",
      "Resized/Cropped Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 1, 172])\n",
      " torch.Size([3, 512, 1, 172])\n",
      "img.shape in collator:  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  67%|██████▋   | 2/3 [00:00<00:00,  7.24it/s, val_loss=3.84, val_ca=0, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 39, 153])\n",
      "img.shape in collator:  torch.Size([3, 77, 160])\n",
      "img.shape in collator:  torch.Size([3, 239, 459])\n",
      "img.shape in collator:  torch.Size([3, 108, 243])\n",
      "img.shape in collator:  torch.Size([3, 638, 1190])\n",
      "img.shape in collator:  torch.Size([3, 143, 230])\n",
      "img.shape in collator:  torch.Size([3, 84, 289])\n",
      "img.shape in collator:  torch.Size([3, 86, 204])\n",
      "img.shape in collator:  torch.Size([3, 71, 225])\n",
      "img.shape in collator:  torch.Size([3, 84, 188])\n",
      "img.shape in collator:  torch.Size([3, 65, 71])\n",
      "img.shape in collator:  torch.Size([3, 68, 155])\n",
      "Conv Size: torch.Size([3, 512, 38, 298])\n",
      "Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 1, 298])\n",
      " Conv Size:torch.Size([3, 512, 38, 298])\n",
      "Resized/Cropped Conv Size:Conv Size: torch.Size([3, 512, 38, 298])\n",
      " torch.Size([3, 512, 38, 298])\n",
      " Resized/Cropped Conv Size: torch.Size([3, 512, 1, 298])\n",
      "torch.Size([3, 512, 1, 298])Resized/Cropped Conv Size: torch.Size([3, 512, 1, 298])\n",
      "\n",
      "img.shape in collator:  torch.Size([3, 99, 296])\n",
      "img.shape in collator:  torch.Size([3, 122, 274])\n",
      "img.shape in collator:  torch.Size([3, 203, 480])\n",
      "img.shape in collator:  torch.Size([3, 47, 150])\n",
      "img.shape in collator:  torch.Size([3, 56, 216])\n",
      "img.shape in collator:  torch.Size([3, 58, 119])\n",
      "img.shape in collator:  torch.Size([3, 61, 260])\n",
      "img.shape in collator:  torch.Size([3, 103, 485])\n",
      "img.shape in collator:  torch.Size([3, 71, 248])\n",
      "img.shape in collator:  torch.Size([3, 81, 222])\n",
      "Conv Size: torch.Size([3, 512, 11, 122])\n",
      "Resized/Cropped Conv Size:Conv Size: torch.Size([3, 512, 1, 122])\n",
      " torch.Size([3, 512, 11, 122])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 3/3 [00:00<00:00,  7.48it/s, val_loss=4.44, val_ca=0, val_wa=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv Size: torch.Size([3, 512, 11, 122])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 122])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 122])\n",
      "Conv Size: torch.Size([1, 512, 11, 122])\n",
      "Resized/Cropped Conv Size: torch.Size([1, 512, 1, 122])\n",
      "4.068390607833862\n",
      "Validation loss decreased (inf --> 4.068391).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[4] Training:  17%|█▋        | 2/12 [00:00<00:01,  9.79it/s, loss=3.92, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape in collator:  torch.Size([3, 53, 221])\n",
      "img.shape in collator:  torch.Size([3, 89, 313])\n",
      "img.shape in collator:  torch.Size([3, 48, 142])\n",
      "img.shape in collator:  torch.Size([3, 74, 133])\n",
      "img.shape in collator:  torch.Size([3, 59, 230])\n",
      "img.shape in collator:  torch.Size([3, 59, 151])\n",
      "img.shape in collator:  torch.Size([3, 133, 272])\n",
      "img.shape in collator:  torch.Size([3, 107, 310])\n",
      "img.shape in collator:  torch.Size([3, 62, 249])\n",
      "img.shape in collator:  torch.Size([3, 63, 223])\n",
      "img.shape in collator:  torch.Size([3, 36, 72])\n",
      "img.shape in collator:  torch.Size([3, 56, 143])\n",
      "Conv Size: torch.Size([3, 512, 7, 79])\n",
      "Resized/Cropped Conv Size:Conv Size: torch.Size([3, 512, 1, 79])\n",
      "Conv Size:  torch.Size([3, 512, 7, 79])\n",
      "Conv Size:torch.Size([3, 512, 7, 79])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 7, 79])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 79])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 79])\n",
      " torch.Size([3, 512, 1, 79])\n",
      "img.shape in collator:  torch.Size([3, 91, 295])\n",
      "img.shape in collator:  torch.Size([3, 93, 402])\n",
      "img.shape in collator:  torch.Size([3, 47, 124])\n",
      "img.shape in collator:  torch.Size([3, 72, 198])\n",
      "img.shape in collator:  torch.Size([3, 69, 212])\n",
      "img.shape in collator:  torch.Size([3, 48, 134])\n",
      "img.shape in collator:  torch.Size([3, 104, 370])\n",
      "img.shape in collator:  torch.Size([3, 62, 210])\n",
      "img.shape in collator:  torch.Size([3, 55, 124])\n",
      "img.shape in collator:  torch.Size([3, 97, 276])\n",
      "img.shape in collator:  torch.Size([3, 57, 177])\n",
      "img.shape in collator:  torch.Size([3, 94, 280])\n",
      "Conv Size: torch.Size([3, 512, 5, 101])\n",
      "Conv Size:Conv Size: torch.Size([3, 512, 5, 101])\n",
      " torch.Size([3, 512, 5, 101])\n",
      "Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 1, 101])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 101])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 5, 101])\n",
      "Resized/Cropped Conv Size:  torch.Size([3, 512, 1, 101])\n",
      "torch.Size([3, 512, 1, 101])\n",
      "img.shape in collator:  torch.Size([3, 152, 269])\n",
      "img.shape in collator:  torch.Size([3, 97, 207])\n",
      "img.shape in collator:  torch.Size([3, 43, 74])\n",
      "img.shape in collator:  torch.Size([3, 154, 406])\n",
      "img.shape in collator:  torch.Size([3, 62, 181])\n",
      "img.shape in collator:  torch.Size([3, 83, 225])\n",
      "img.shape in collator:  torch.Size([3, 76, 169])\n",
      "img.shape in collator:  torch.Size([3, 62, 255])\n",
      "img.shape in collator:  torch.Size([3, 294, 1374])\n",
      "img.shape in collator:  torch.Size([3, 66, 161])\n",
      "img.shape in collator:  torch.Size([3, 48, 209])\n",
      "img.shape in collator:  torch.Size([3, 52, 168])\n",
      "Conv Size:Conv Size:Conv Size: torch.Size([3, 512, 17, 344])\n",
      " torch.Size([3, 512, 17, 344])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 344])\n",
      " torch.Size([3, 512, 17, 344])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 344])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 344])\n",
      "Conv Size: torch.Size([3, 512, 17, 344])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 344])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[4] Training:  33%|███▎      | 4/12 [00:00<00:01,  6.53it/s, loss=3.68, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape in collator:  torch.Size([3, 51, 111])\n",
      "img.shape in collator:  torch.Size([3, 46, 124])\n",
      "img.shape in collator:  torch.Size([3, 47, 119])\n",
      "img.shape in collator:  torch.Size([3, 64, 127])\n",
      "img.shape in collator:  torch.Size([3, 46, 105])\n",
      "img.shape in collator:  torch.Size([3, 59, 80])\n",
      "img.shape in collator:  torch.Size([3, 59, 129])\n",
      "img.shape in collator:  torch.Size([3, 75, 215])\n",
      "img.shape in collator:  torch.Size([3, 56, 165])\n",
      "img.shape in collator:  torch.Size([3, 50, 118])\n",
      "img.shape in collator:  torch.Size([3, 64, 202])\n",
      "img.shape in collator:  torch.Size([3, 154, 416])\n",
      "Conv Size:Conv Size: torch.Size([3, 512, 8, 105])\n",
      " Resized/Cropped Conv Size:Conv Size: torch.Size([3, 512, 8, 105])\n",
      "Conv Size:torch.Size([3, 512, 8, 105])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 105])\n",
      "  Resized/Cropped Conv Size:torch.Size([3, 512, 1, 105])\n",
      "torch.Size([3, 512, 8, 105])\n",
      " Resized/Cropped Conv Size:torch.Size([3, 512, 1, 105])\n",
      " torch.Size([3, 512, 1, 105])\n",
      "img.shape in collator:  torch.Size([3, 93, 328])\n",
      "img.shape in collator:  torch.Size([3, 98, 206])\n",
      "img.shape in collator:  torch.Size([3, 91, 214])\n",
      "img.shape in collator:  torch.Size([3, 77, 157])\n",
      "img.shape in collator:  torch.Size([3, 66, 205])\n",
      "img.shape in collator:  torch.Size([3, 60, 159])\n",
      "img.shape in collator:  torch.Size([3, 75, 281])\n",
      "img.shape in collator:  torch.Size([3, 78, 196])\n",
      "img.shape in collator:  torch.Size([3, 66, 176])\n",
      "img.shape in collator:  torch.Size([3, 61, 198])\n",
      "img.shape in collator:  torch.Size([3, 509, 1158])\n",
      "img.shape in collator:  torch.Size([3, 52, 210])\n",
      "Conv Size:Conv Size: torch.Size([3, 512, 30, 290])\n",
      " torch.Size([3, 512, 30, 290])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 290])\n",
      "Conv Size:Resized/Cropped Conv Size:  torch.Size([3, 512, 1, 290])\n",
      "torch.Size([3, 512, 30, 290])\n",
      "Resized/Cropped Conv Size:Conv Size:  torch.Size([3, 512, 30, 290])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 290])\n",
      "torch.Size([3, 512, 1, 290])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[4] Training:  42%|████▏     | 5/12 [00:01<00:01,  4.45it/s, loss=5.95, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape in collator:  torch.Size([3, 39, 179])\n",
      "img.shape in collator:  torch.Size([3, 177, 747])\n",
      "img.shape in collator:  torch.Size([3, 115, 209])\n",
      "img.shape in collator:  torch.Size([3, 60, 110])\n",
      "img.shape in collator:  torch.Size([3, 526, 2043])\n",
      "img.shape in collator:  torch.Size([3, 62, 137])\n",
      "img.shape in collator:  torch.Size([3, 56, 126])\n",
      "img.shape in collator:  torch.Size([3, 47, 139])\n",
      "img.shape in collator:  torch.Size([3, 301, 528])\n",
      "img.shape in collator:  torch.Size([3, 110, 153])\n",
      "img.shape in collator:  torch.Size([3, 59, 194])\n",
      "img.shape in collator:  torch.Size([3, 110, 399])\n",
      "Conv Size: torch.Size([3, 512, 31, 511])\n",
      "Conv Size: torch.Size([3, 512, 31, 511])\n",
      "Conv Size:Resized/Cropped Conv Size:Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 1, 511])\n",
      "  torch.Size([3, 512, 31, 511])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 31, 511])\n",
      "torch.Size([3, 512, 1, 511])Resized/Cropped Conv Size: torch.Size([3, 512, 1, 511])\n",
      " torch.Size([3, 512, 1, 511])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[4] Training:  50%|█████     | 6/12 [00:01<00:02,  2.79it/s, loss=9.33, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape in collator:  torch.Size([3, 80, 268])\n",
      "img.shape in collator:  torch.Size([3, 239, 813])\n",
      "img.shape in collator:  torch.Size([3, 133, 237])\n",
      "img.shape in collator:  torch.Size([3, 83, 209])\n",
      "img.shape in collator:  torch.Size([3, 147, 690])\n",
      "img.shape in collator:  torch.Size([3, 56, 144])\n",
      "img.shape in collator:  torch.Size([3, 72, 152])\n",
      "img.shape in collator:  torch.Size([3, 388, 1301])\n",
      "img.shape in collator:  torch.Size([3, 52, 159])\n",
      "img.shape in collator:  torch.Size([3, 56, 113])\n",
      "img.shape in collator:  torch.Size([3, 35, 110])\n",
      "img.shape in collator:  torch.Size([3, 74, 161])\n",
      "Conv Size:Conv Size: torch.Size([3, 512, 23, 326])\n",
      "Conv Size:  torch.Size([3, 512, 23, 326])\n",
      "Resized/Cropped Conv Size:Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 1, 326])\n",
      "torch.Size([3, 512, 23, 326])\n",
      " torch.Size([3, 512, 23, 326])Resized/Cropped Conv Size: torch.Size([3, 512, 1, 326])\n",
      "\n",
      " Resized/Cropped Conv Size: torch.Size([3, 512, 1, 326])\n",
      "torch.Size([3, 512, 1, 326])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[4] Training:  67%|██████▋   | 8/12 [00:02<00:01,  3.17it/s, loss=3.75, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape in collator:  torch.Size([3, 39, 118])\n",
      "img.shape in collator:  torch.Size([3, 49, 180])\n",
      "img.shape in collator:  torch.Size([3, 71, 183])\n",
      "img.shape in collator:  torch.Size([3, 45, 128])\n",
      "img.shape in collator:  torch.Size([3, 49, 162])\n",
      "img.shape in collator:  torch.Size([3, 37, 189])\n",
      "img.shape in collator:  torch.Size([3, 109, 199])\n",
      "img.shape in collator:  torch.Size([3, 43, 149])\n",
      "img.shape in collator:  torch.Size([3, 54, 148])\n",
      "img.shape in collator:  torch.Size([3, 334, 657])\n",
      "img.shape in collator:  torch.Size([3, 64, 183])\n",
      "img.shape in collator:  torch.Size([3, 68, 216])\n",
      "Conv Size: torch.Size([3, 512, 19, 165])\n",
      "Conv Size: torch.Size([3, 512, 19, 165])\n",
      "Conv Size: Resized/Cropped Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 1, 165])\n",
      "torch.Size([3, 512, 19, 165])\n",
      " torch.Size([3, 512, 1, 165])Resized/Cropped Conv Size: torch.Size([3, 512, 1, 165])\n",
      "\n",
      "Conv Size: torch.Size([3, 512, 19, 165])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 165])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[4] Training:  75%|███████▌  | 9/12 [00:02<00:00,  3.72it/s, loss=3.58, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape in collator:  torch.Size([3, 52, 139])\n",
      "img.shape in collator:  torch.Size([3, 74, 157])\n",
      "img.shape in collator:  torch.Size([3, 73, 283])\n",
      "img.shape in collator:  torch.Size([3, 129, 312])\n",
      "img.shape in collator:  torch.Size([3, 58, 176])\n",
      "img.shape in collator:  torch.Size([3, 67, 291])\n",
      "img.shape in collator:  torch.Size([3, 77, 239])\n",
      "img.shape in collator:  torch.Size([3, 61, 97])\n",
      "img.shape in collator:  torch.Size([3, 312, 477])\n",
      "img.shape in collator:  torch.Size([3, 74, 205])\n",
      "img.shape in collator:  torch.Size([3, 86, 291])\n",
      "img.shape in collator:  torch.Size([3, 236, 489])\n",
      "Conv Size:Conv Size: torch.Size([3, 512, 18, 123])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 18, 123])\n",
      " torch.Size([3, 512, 1, 123])\n",
      "Conv Size:Resized/Cropped Conv Size:  torch.Size([3, 512, 18, 123])torch.Size([3, 512, 1, 123])\n",
      "\n",
      "Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 18, 123])\n",
      " Resized/Cropped Conv Size: torch.Size([3, 512, 1, 123])\n",
      "torch.Size([3, 512, 1, 123])\n",
      "img.shape in collator:  torch.Size([3, 208, 283])\n",
      "img.shape in collator:  torch.Size([3, 49, 120])\n",
      "img.shape in collator:  torch.Size([3, 110, 403])\n",
      "img.shape in collator:  torch.Size([3, 48, 67])\n",
      "img.shape in collator:  torch.Size([3, 56, 155])\n",
      "img.shape in collator:  torch.Size([3, 127, 479])\n",
      "img.shape in collator:  torch.Size([3, 163, 464])\n",
      "img.shape in collator:  torch.Size([3, 73, 203])\n",
      "img.shape in collator:  torch.Size([3, 74, 173])\n",
      "img.shape in collator:  torch.Size([3, 57, 188])\n",
      "img.shape in collator:  torch.Size([3, 74, 217])\n",
      "img.shape in collator:  torch.Size([3, 199, 408])\n",
      "Conv Size: torch.Size([3, 512, 12, 120])\n",
      "Conv Size: Conv Size:Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 1, 120])\n",
      "torch.Size([3, 512, 12, 120])\n",
      " torch.Size([3, 512, 12, 120])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 12, 120])\n",
      "Resized/Cropped Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 1, 120])\n",
      " torch.Size([3, 512, 1, 120])\n",
      " torch.Size([3, 512, 1, 120])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[4] Training:  92%|█████████▏| 11/12 [00:02<00:00,  5.15it/s, loss=3.69, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape in collator:  torch.Size([3, 132, 373])\n",
      "img.shape in collator:  torch.Size([3, 97, 459])\n",
      "img.shape in collator:  torch.Size([3, 52, 145])\n",
      "img.shape in collator:  torch.Size([3, 69, 134])\n",
      "img.shape in collator:  torch.Size([3, 62, 201])\n",
      "img.shape in collator:  torch.Size([3, 52, 125])\n",
      "img.shape in collator:  torch.Size([3, 114, 144])\n",
      "img.shape in collator:  torch.Size([3, 63, 238])\n",
      "img.shape in collator:  torch.Size([3, 89, 210])\n",
      "img.shape in collator:  torch.Size([3, 52, 168])\n",
      "img.shape in collator:  torch.Size([3, 75, 300])\n",
      "img.shape in collator:  torch.Size([3, 62, 168])\n",
      "Conv Size: torch.Size([3, 512, 7, 115])\n",
      "Conv Size:Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 1, 115])\n",
      "  torch.Size([3, 512, 7, 115])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 115])\n",
      "torch.Size([3, 512, 7, 115])\n",
      "Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 7, 115])\n",
      " Resized/Cropped Conv Size: torch.Size([3, 512, 1, 115])\n",
      "torch.Size([3, 512, 1, 115])\n",
      "img.shape in collator:  torch.Size([3, 491, 2126])\n",
      "Conv Size: torch.Size([1, 512, 29, 532])\n",
      "Resized/Cropped Conv Size: torch.Size([1, 512, 1, 532])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[4] Training: 100%|██████████| 12/12 [00:02<00:00,  4.07it/s, loss=4.17, train_ca=nan, train_wa=nan]\n",
      "Validating:  33%|███▎      | 1/3 [00:00<00:00,  9.98it/s, val_loss=3.42, val_ca=0, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape in collator:  torch.Size([3, 167, 247])\n",
      "img.shape in collator:  torch.Size([3, 293, 684])\n",
      "img.shape in collator:  torch.Size([3, 44, 110])\n",
      "img.shape in collator:  torch.Size([3, 47, 144])\n",
      "img.shape in collator:  torch.Size([3, 59, 177])\n",
      "img.shape in collator:  torch.Size([3, 114, 550])\n",
      "img.shape in collator:  torch.Size([3, 108, 351])\n",
      "img.shape in collator:  torch.Size([3, 68, 164])\n",
      "img.shape in collator:  torch.Size([3, 110, 203])\n",
      "img.shape in collator:  torch.Size([3, 33, 97])\n",
      "img.shape in collator:  torch.Size([3, 68, 209])\n",
      "img.shape in collator:  torch.Size([3, 87, 243])\n",
      "Conv Size: torch.Size([3, 512, 17, 172])\n",
      "Conv Size:Conv Size: torch.Size([3, 512, 17, 172])\n",
      "Resized/Cropped Conv Size: Resized/Cropped Conv Size: torch.Size([3, 512, 1, 172])\n",
      "Conv Size:  torch.Size([3, 512, 17, 172])\n",
      "Resized/Cropped Conv Size:torch.Size([3, 512, 17, 172])torch.Size([3, 512, 1, 172])\n",
      "\n",
      " Resized/Cropped Conv Size: torch.Size([3, 512, 1, 172])\n",
      "torch.Size([3, 512, 1, 172])\n",
      "img.shape in collator:  torch.Size([3, 39, 153])\n",
      "img.shape in collator:  torch.Size([3, 77, 160])\n",
      "img.shape in collator:  torch.Size([3, 239, 459])\n",
      "img.shape in collator:  torch.Size([3, 108, 243])\n",
      "img.shape in collator:  torch.Size([3, 638, 1190])\n",
      "img.shape in collator:  torch.Size([3, 143, 230])\n",
      "img.shape in collator:  torch.Size([3, 84, 289])\n",
      "img.shape in collator:  torch.Size([3, 86, 204])\n",
      "img.shape in collator:  torch.Size([3, 71, 225])\n",
      "img.shape in collator:  torch.Size([3, 84, 188])\n",
      "img.shape in collator:  torch.Size([3, 65, 71])\n",
      "img.shape in collator:  torch.Size([3, 68, 155])\n",
      "Conv Size:Conv Size: torch.Size([3, 512, 38, 298])\n",
      " torch.Size([3, 512, 38, 298])\n",
      "Conv Size: torch.Size([3, 512, 38, 298])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 298])\n",
      "Resized/Cropped Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 1, 298])\n",
      " Conv Size:torch.Size([3, 512, 1, 298])\n",
      " torch.Size([3, 512, 38, 298])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 298])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 3/3 [00:00<00:00,  7.47it/s, val_loss=3.76, val_ca=0, val_wa=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape in collator:  torch.Size([3, 99, 296])\n",
      "img.shape in collator:  torch.Size([3, 122, 274])\n",
      "img.shape in collator:  torch.Size([3, 203, 480])\n",
      "img.shape in collator:  torch.Size([3, 47, 150])\n",
      "img.shape in collator:  torch.Size([3, 56, 216])\n",
      "img.shape in collator:  torch.Size([3, 58, 119])\n",
      "img.shape in collator:  torch.Size([3, 61, 260])\n",
      "img.shape in collator:  torch.Size([3, 103, 485])\n",
      "img.shape in collator:  torch.Size([3, 71, 248])\n",
      "img.shape in collator:  torch.Size([3, 81, 222])\n",
      "Conv Size: torch.Size([3, 512, 11, 122])\n",
      "Conv Size: Conv Size: torch.Size([3, 512, 11, 122])\n",
      "Resized/Cropped Conv Size:Conv Size:torch.Size([3, 512, 11, 122])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 122])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 122])\n",
      " torch.Size([1, 512, 11, 122])\n",
      " torch.Size([3, 512, 1, 122])Resized/Cropped Conv Size:\n",
      " torch.Size([1, 512, 1, 122])\n",
      "3.8325111071268716\n",
      "Validation loss decreased (4.068391 --> 3.832511).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [1]/[4] Training:   8%|▊         | 1/12 [00:00<00:01,  5.79it/s, loss=3.47, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape in collator:  torch.Size([3, 52, 139])\n",
      "img.shape in collator:  torch.Size([3, 177, 747])\n",
      "img.shape in collator:  torch.Size([3, 61, 97])\n",
      "img.shape in collator:  torch.Size([3, 43, 149])\n",
      "img.shape in collator:  torch.Size([3, 57, 177])\n",
      "img.shape in collator:  torch.Size([3, 59, 194])\n",
      "img.shape in collator:  torch.Size([3, 57, 188])\n",
      "img.shape in collator:  torch.Size([3, 74, 133])\n",
      "img.shape in collator:  torch.Size([3, 208, 283])\n",
      "img.shape in collator:  torch.Size([3, 48, 67])\n",
      "img.shape in collator:  torch.Size([3, 36, 72])\n",
      "img.shape in collator:  torch.Size([3, 77, 239])\n",
      "Conv Size: torch.Size([3, 512, 12, 187])\n",
      "Conv Size:Conv Size: torch.Size([3, 512, 12, 187])\n",
      " torch.Size([3, 512, 12, 187])Resized/Cropped Conv Size: torch.Size([3, 512, 1, 187])\n",
      "Conv Size:\n",
      " torch.Size([3, 512, 12, 187])\n",
      "Resized/Cropped Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 1, 187])\n",
      " torch.Size([3, 512, 1, 187])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 187])\n",
      "img.shape in collator:  torch.Size([3, 56, 126])\n",
      "img.shape in collator:  torch.Size([3, 62, 181])\n",
      "img.shape in collator:  torch.Size([3, 52, 159])\n",
      "img.shape in collator:  torch.Size([3, 67, 291])\n",
      "img.shape in collator:  torch.Size([3, 107, 310])\n",
      "img.shape in collator:  torch.Size([3, 93, 402])\n",
      "img.shape in collator:  torch.Size([3, 69, 212])\n",
      "img.shape in collator:  torch.Size([3, 78, 196])\n",
      "img.shape in collator:  torch.Size([3, 56, 165])\n",
      "img.shape in collator:  torch.Size([3, 58, 176])\n",
      "img.shape in collator:  torch.Size([3, 93, 328])\n",
      "img.shape in collator:  torch.Size([3, 52, 168])\n",
      "Conv Size:Conv Size: torch.Size([3, 512, 5, 101])\n",
      "Resized/Cropped Conv Size:Conv Size: torch.Size([3, 512, 5, 101])\n",
      " torch.Size([3, 512, 5, 101])\n",
      "Conv Size: torch.Size([3, 512, 1, 101])\n",
      " torch.Size([3, 512, 5, 101])\n",
      "Resized/Cropped Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 1, 101])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 101])\n",
      " torch.Size([3, 512, 1, 101])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [1]/[4] Training:  33%|███▎      | 4/12 [00:00<00:01,  7.57it/s, loss=3.75, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape in collator:  torch.Size([3, 48, 209])\n",
      "img.shape in collator:  torch.Size([3, 62, 137])\n",
      "img.shape in collator:  torch.Size([3, 73, 283])\n",
      "img.shape in collator:  torch.Size([3, 74, 217])\n",
      "img.shape in collator:  torch.Size([3, 56, 155])\n",
      "img.shape in collator:  torch.Size([3, 94, 280])\n",
      "img.shape in collator:  torch.Size([3, 109, 199])\n",
      "img.shape in collator:  torch.Size([3, 55, 124])\n",
      "img.shape in collator:  torch.Size([3, 110, 403])\n",
      "img.shape in collator:  torch.Size([3, 114, 144])\n",
      "img.shape in collator:  torch.Size([3, 62, 201])\n",
      "img.shape in collator:  torch.Size([3, 104, 370])\n",
      "Conv Size: torch.Size([3, 512, 6, 101])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 101])\n",
      "Conv Size: torch.Size([3, 512, 6, 101])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 101])\n",
      "Conv Size: torch.Size([3, 512, 6, 101])\n",
      "Conv Size: torch.Size([3, 512, 6, 101])\n",
      "Resized/Cropped Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 1, 101])\n",
      " torch.Size([3, 512, 1, 101])\n",
      "img.shape in collator:  torch.Size([3, 154, 406])\n",
      "img.shape in collator:  torch.Size([3, 47, 119])\n",
      "img.shape in collator:  torch.Size([3, 110, 399])\n",
      "img.shape in collator:  torch.Size([3, 76, 169])\n",
      "img.shape in collator:  torch.Size([3, 35, 110])\n",
      "img.shape in collator:  torch.Size([3, 47, 139])\n",
      "img.shape in collator:  torch.Size([3, 62, 210])\n",
      "img.shape in collator:  torch.Size([3, 66, 205])\n",
      "img.shape in collator:  torch.Size([3, 86, 291])\n",
      "img.shape in collator:  torch.Size([3, 71, 183])\n",
      "img.shape in collator:  torch.Size([3, 48, 142])\n",
      "img.shape in collator:  torch.Size([3, 64, 202])\n",
      "Conv Size: torch.Size([3, 512, 8, 102])\n",
      "Conv Size: torch.Size([3, 512, 8, 102])\n",
      "Resized/Cropped Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 1, 102])\n",
      " torch.Size([3, 512, 1, 102])\n",
      "Conv Size:Conv Size: torch.Size([3, 512, 8, 102])\n",
      " torch.Size([3, 512, 8, 102])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 102])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 102])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [1]/[4] Training:  42%|████▏     | 5/12 [00:00<00:01,  6.66it/s, loss=3.64, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape in collator:  torch.Size([3, 59, 151])\n",
      "img.shape in collator:  torch.Size([3, 39, 118])\n",
      "img.shape in collator:  torch.Size([3, 239, 813])\n",
      "img.shape in collator:  torch.Size([3, 91, 214])\n",
      "img.shape in collator:  torch.Size([3, 133, 272])\n",
      "img.shape in collator:  torch.Size([3, 163, 464])\n",
      "img.shape in collator:  torch.Size([3, 56, 113])\n",
      "img.shape in collator:  torch.Size([3, 49, 180])\n",
      "img.shape in collator:  torch.Size([3, 52, 125])\n",
      "img.shape in collator:  torch.Size([3, 39, 179])\n",
      "img.shape in collator:  torch.Size([3, 154, 416])\n",
      "img.shape in collator:  torch.Size([3, 74, 157])\n",
      "Conv Size: torch.Size([3, 512, 13, 204])\n",
      "Conv Size: torch.Size([3, 512, 13, 204])\n",
      "Resized/Cropped Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 1, 204])\n",
      " torch.Size([3, 512, 1, 204])\n",
      "Conv Size: torch.Size([3, 512, 13, 204])\n",
      "Resized/Cropped Conv Size:Conv Size: torch.Size([3, 512, 1, 204])\n",
      " torch.Size([3, 512, 13, 204])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 204])\n",
      "img.shape in collator:  torch.Size([3, 97, 276])\n",
      "img.shape in collator:  torch.Size([3, 48, 134])\n",
      "img.shape in collator:  torch.Size([3, 147, 690])\n",
      "img.shape in collator:  torch.Size([3, 47, 124])\n",
      "img.shape in collator:  torch.Size([3, 59, 230])\n",
      "img.shape in collator:  torch.Size([3, 64, 183])\n",
      "img.shape in collator:  torch.Size([3, 152, 269])\n",
      "img.shape in collator:  torch.Size([3, 68, 216])\n",
      "img.shape in collator:  torch.Size([3, 83, 209])\n",
      "img.shape in collator:  torch.Size([3, 334, 657])\n",
      "img.shape in collator:  torch.Size([3, 60, 159])\n",
      "img.shape in collator:  torch.Size([3, 73, 203])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [1]/[4] Training:  50%|█████     | 6/12 [00:00<00:00,  6.00it/s, loss=3.52, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv Size:Conv Size: torch.Size([3, 512, 19, 173])\n",
      " torch.Size([3, 512, 19, 173])\n",
      "Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 1, 173])\n",
      "Resized/Cropped Conv Size: Conv Size: torch.Size([3, 512, 19, 173])\n",
      " torch.Size([3, 512, 19, 173])\n",
      "Resized/Cropped Conv Size:torch.Size([3, 512, 1, 173])\n",
      " torch.Size([3, 512, 1, 173])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 173])\n",
      "img.shape in collator:  torch.Size([3, 43, 74])\n",
      "img.shape in collator:  torch.Size([3, 56, 143])\n",
      "img.shape in collator:  torch.Size([3, 199, 408])\n",
      "img.shape in collator:  torch.Size([3, 75, 215])\n",
      "img.shape in collator:  torch.Size([3, 52, 145])\n",
      "img.shape in collator:  torch.Size([3, 37, 189])\n",
      "img.shape in collator:  torch.Size([3, 59, 80])\n",
      "img.shape in collator:  torch.Size([3, 61, 198])\n",
      "img.shape in collator:  torch.Size([3, 110, 153])\n",
      "img.shape in collator:  torch.Size([3, 509, 1158])\n",
      "img.shape in collator:  torch.Size([3, 49, 162])\n",
      "img.shape in collator:  torch.Size([3, 52, 168])\n",
      "Conv Size:Conv Size: torch.Size([3, 512, 30, 290])\n",
      " Conv Size: torch.Size([3, 512, 30, 290])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 290])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 290])\n",
      "torch.Size([3, 512, 30, 290])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 290])\n",
      "Conv Size: torch.Size([3, 512, 30, 290])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 290])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [1]/[4] Training:  67%|██████▋   | 8/12 [00:01<00:00,  4.71it/s, loss=3.65, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape in collator:  torch.Size([3, 51, 111])\n",
      "img.shape in collator:  torch.Size([3, 69, 134])\n",
      "img.shape in collator:  torch.Size([3, 127, 479])\n",
      "img.shape in collator:  torch.Size([3, 97, 459])\n",
      "img.shape in collator:  torch.Size([3, 89, 210])\n",
      "img.shape in collator:  torch.Size([3, 91, 295])\n",
      "img.shape in collator:  torch.Size([3, 72, 198])\n",
      "img.shape in collator:  torch.Size([3, 50, 118])\n",
      "img.shape in collator:  torch.Size([3, 80, 268])\n",
      "img.shape in collator:  torch.Size([3, 89, 313])\n",
      "img.shape in collator:  torch.Size([3, 312, 477])\n",
      "img.shape in collator:  torch.Size([3, 97, 207])\n",
      "Conv Size: torch.Size([3, 512, 18, 120])\n",
      "Conv Size:Conv Size:Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 18, 120])\n",
      " torch.Size([3, 512, 1, 120])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 120])\n",
      " torch.Size([3, 512, 18, 120])\n",
      " Resized/Cropped Conv Size:torch.Size([3, 512, 18, 120])\n",
      " torch.Size([3, 512, 1, 120])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 120])\n",
      "img.shape in collator:  torch.Size([3, 115, 209])\n",
      "img.shape in collator:  torch.Size([3, 45, 128])\n",
      "img.shape in collator:  torch.Size([3, 236, 489])\n",
      "img.shape in collator:  torch.Size([3, 66, 176])\n",
      "img.shape in collator:  torch.Size([3, 74, 205])\n",
      "img.shape in collator:  torch.Size([3, 77, 157])\n",
      "img.shape in collator:  torch.Size([3, 46, 124])\n",
      "img.shape in collator:  torch.Size([3, 59, 129])\n",
      "img.shape in collator:  torch.Size([3, 62, 249])\n",
      "img.shape in collator:  torch.Size([3, 52, 210])\n",
      "img.shape in collator:  torch.Size([3, 74, 161])\n",
      "img.shape in collator:  torch.Size([3, 301, 528])\n",
      "Conv Size:Conv Size: torch.Size([3, 512, 17, 133])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 17, 133])\n",
      "Resized/Cropped Conv Size:Conv Size:Conv Size: torch.Size([3, 512, 17, 133])\n",
      " torch.Size([3, 512, 1, 133])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 133])\n",
      " torch.Size([3, 512, 1, 133]) torch.Size([3, 512, 17, 133])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 133])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [1]/[4] Training:  75%|███████▌  | 9/12 [00:01<00:00,  5.06it/s, loss=3.48, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape in collator:  torch.Size([3, 53, 221])\n",
      "img.shape in collator:  torch.Size([3, 54, 148])\n",
      "img.shape in collator:  torch.Size([3, 491, 2126])\n",
      "img.shape in collator:  torch.Size([3, 129, 312])\n",
      "img.shape in collator:  torch.Size([3, 388, 1301])\n",
      "img.shape in collator:  torch.Size([3, 56, 144])\n",
      "img.shape in collator:  torch.Size([3, 64, 127])\n",
      "img.shape in collator:  torch.Size([3, 62, 168])\n",
      "img.shape in collator:  torch.Size([3, 83, 225])\n",
      "img.shape in collator:  torch.Size([3, 46, 105])\n",
      "img.shape in collator:  torch.Size([3, 66, 161])\n",
      "img.shape in collator:  torch.Size([3, 294, 1374])\n",
      "Conv Size:Conv Size: torch.Size([3, 512, 29, 532])\n",
      "Conv Size: torch.Size([3, 512, 29, 532])\n",
      " torch.Size([3, 512, 29, 532])\n",
      "Resized/Cropped Conv Size:Resized/Cropped Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 1, 532])\n",
      " torch.Size([3, 512, 1, 532])\n",
      " torch.Size([3, 512, 1, 532])\n",
      "Conv Size: torch.Size([3, 512, 29, 532])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 532])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [1]/[4] Training:  83%|████████▎ | 10/12 [00:02<00:00,  2.91it/s, loss=5, train_ca=0, train_wa=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape in collator:  torch.Size([3, 75, 281])\n",
      "img.shape in collator:  torch.Size([3, 132, 373])\n",
      "img.shape in collator:  torch.Size([3, 62, 255])\n",
      "img.shape in collator:  torch.Size([3, 74, 173])\n",
      "img.shape in collator:  torch.Size([3, 63, 238])\n",
      "img.shape in collator:  torch.Size([3, 75, 300])\n",
      "img.shape in collator:  torch.Size([3, 526, 2043])\n",
      "img.shape in collator:  torch.Size([3, 60, 110])\n",
      "img.shape in collator:  torch.Size([3, 133, 237])\n",
      "img.shape in collator:  torch.Size([3, 72, 152])\n",
      "img.shape in collator:  torch.Size([3, 98, 206])\n",
      "img.shape in collator:  torch.Size([3, 49, 120])\n",
      "Conv Size:Conv Size:Conv Size: torch.Size([3, 512, 31, 511])\n",
      " torch.Size([3, 512, 31, 511])\n",
      "Conv Size: torch.Size([3, 512, 31, 511])\n",
      " torch.Size([3, 512, 31, 511])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 511])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 511])\n",
      "Resized/Cropped Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 1, 511])\n",
      " torch.Size([3, 512, 1, 511])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [1]/[4] Training: 100%|██████████| 12/12 [00:03<00:00,  3.98it/s, loss=3.8, train_ca=nan, train_wa=nan]\n",
      "Validating:  33%|███▎      | 1/3 [00:00<00:00,  9.80it/s, val_loss=3.55, val_ca=0, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape in collator:  torch.Size([3, 63, 223])\n",
      "Conv Size: torch.Size([1, 512, 2, 56])\n",
      "Resized/Cropped Conv Size: torch.Size([1, 512, 1, 56])\n",
      "img.shape in collator:  torch.Size([3, 167, 247])\n",
      "img.shape in collator:  torch.Size([3, 293, 684])\n",
      "img.shape in collator:  torch.Size([3, 44, 110])\n",
      "img.shape in collator:  torch.Size([3, 47, 144])\n",
      "img.shape in collator:  torch.Size([3, 59, 177])\n",
      "img.shape in collator:  torch.Size([3, 114, 550])\n",
      "img.shape in collator:  torch.Size([3, 108, 351])\n",
      "img.shape in collator:  torch.Size([3, 68, 164])\n",
      "img.shape in collator:  torch.Size([3, 110, 203])\n",
      "img.shape in collator:  torch.Size([3, 33, 97])\n",
      "img.shape in collator:  torch.Size([3, 68, 209])\n",
      "img.shape in collator:  torch.Size([3, 87, 243])\n",
      "Conv Size:Conv Size: torch.Size([3, 512, 17, 172])\n",
      "Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 17, 172])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 172])\n",
      " torch.Size([3, 512, 1, 172])Conv Size: torch.Size([3, 512, 17, 172])\n",
      "\n",
      " Resized/Cropped Conv Size: torch.Size([3, 512, 1, 172])\n",
      "torch.Size([3, 512, 17, 172])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 172])\n",
      "img.shape in collator:  torch.Size([3, 39, 153])\n",
      "img.shape in collator:  torch.Size([3, 77, 160])\n",
      "img.shape in collator:  torch.Size([3, 239, 459])\n",
      "img.shape in collator:  torch.Size([3, 108, 243])\n",
      "img.shape in collator:  torch.Size([3, 638, 1190])\n",
      "img.shape in collator:  torch.Size([3, 143, 230])\n",
      "img.shape in collator:  torch.Size([3, 84, 289])\n",
      "img.shape in collator:  torch.Size([3, 86, 204])\n",
      "img.shape in collator:  torch.Size([3, 71, 225])\n",
      "img.shape in collator:  torch.Size([3, 84, 188])\n",
      "img.shape in collator:  torch.Size([3, 65, 71])\n",
      "img.shape in collator:  torch.Size([3, 68, 155])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  67%|██████▋   | 2/3 [00:00<00:00,  7.30it/s, val_loss=4.07, val_ca=0, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv Size:Conv Size: torch.Size([3, 512, 38, 298])\n",
      " torch.Size([3, 512, 38, 298])\n",
      "Conv Size:Resized/Cropped Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 1, 298])\n",
      " torch.Size([3, 512, 38, 298])\n",
      " torch.Size([3, 512, 1, 298])\n",
      "Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 38, 298])\n",
      " Resized/Cropped Conv Size:torch.Size([3, 512, 1, 298])\n",
      " torch.Size([3, 512, 1, 298])\n",
      "img.shape in collator:  torch.Size([3, 99, 296])\n",
      "img.shape in collator:  torch.Size([3, 122, 274])\n",
      "img.shape in collator:  torch.Size([3, 203, 480])\n",
      "img.shape in collator:  torch.Size([3, 47, 150])\n",
      "img.shape in collator:  torch.Size([3, 56, 216])\n",
      "img.shape in collator:  torch.Size([3, 58, 119])\n",
      "img.shape in collator:  torch.Size([3, 61, 260])\n",
      "img.shape in collator:  torch.Size([3, 103, 485])\n",
      "img.shape in collator:  torch.Size([3, 71, 248])\n",
      "img.shape in collator:  torch.Size([3, 81, 222])\n",
      "Conv Size: torch.Size([3, 512, 11, 122])\n",
      "Conv Size: torch.Size([1, 512, 11, 122])\n",
      "Resized/Cropped Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 1, 122])\n",
      "Conv Size: torch.Size([3, 512, 11, 122])\n",
      " torch.Size([1, 512, 1, 122])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 122])\n",
      "Conv Size: torch.Size([3, 512, 11, 122])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 122])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 3/3 [00:00<00:00,  7.50it/s, val_loss=3.69, val_ca=0, val_wa=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8119551605648465\n",
      "Validation loss decreased (3.832511 --> 3.811955).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [2]/[4] Training:   8%|▊         | 1/12 [00:00<00:01,  8.29it/s, loss=3.76, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape in collator:  torch.Size([3, 110, 403])\n",
      "img.shape in collator:  torch.Size([3, 47, 119])\n",
      "img.shape in collator:  torch.Size([3, 59, 151])\n",
      "img.shape in collator:  torch.Size([3, 208, 283])\n",
      "img.shape in collator:  torch.Size([3, 56, 113])\n",
      "img.shape in collator:  torch.Size([3, 83, 225])\n",
      "img.shape in collator:  torch.Size([3, 73, 203])\n",
      "img.shape in collator:  torch.Size([3, 59, 129])\n",
      "img.shape in collator:  torch.Size([3, 74, 157])\n",
      "img.shape in collator:  torch.Size([3, 48, 134])\n",
      "img.shape in collator:  torch.Size([3, 63, 238])\n",
      "img.shape in collator:  torch.Size([3, 64, 202])\n",
      "Conv Size: torch.Size([3, 512, 12, 101])\n",
      "Conv Size:Conv Size:Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 12, 101])\n",
      " torch.Size([3, 512, 1, 101])\n",
      " torch.Size([3, 512, 12, 101])\n",
      " Resized/Cropped Conv Size:torch.Size([3, 512, 12, 101])\n",
      " torch.Size([3, 512, 1, 101])\n",
      "Resized/Cropped Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 1, 101])\n",
      " torch.Size([3, 512, 1, 101])\n",
      "img.shape in collator:  torch.Size([3, 48, 209])\n",
      "img.shape in collator:  torch.Size([3, 77, 239])\n",
      "img.shape in collator:  torch.Size([3, 133, 237])\n",
      "img.shape in collator:  torch.Size([3, 62, 137])\n",
      "img.shape in collator:  torch.Size([3, 133, 272])\n",
      "img.shape in collator:  torch.Size([3, 46, 124])\n",
      "img.shape in collator:  torch.Size([3, 61, 198])\n",
      "img.shape in collator:  torch.Size([3, 177, 747])\n",
      "img.shape in collator:  torch.Size([3, 55, 124])\n",
      "img.shape in collator:  torch.Size([3, 62, 210])\n",
      "img.shape in collator:  torch.Size([3, 56, 165])\n",
      "img.shape in collator:  torch.Size([3, 72, 152])\n",
      "Conv Size:Conv Size: torch.Size([3, 512, 10, 187])\n",
      "Conv Size: Conv Size: torch.Size([3, 512, 10, 187])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 187])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 187])\n",
      "torch.Size([3, 512, 10, 187])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 10, 187])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 187])\n",
      " torch.Size([3, 512, 1, 187])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [2]/[4] Training:  17%|█▋        | 2/12 [00:00<00:01,  7.51it/s, loss=3.81, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape in collator:  torch.Size([3, 52, 125])\n",
      "img.shape in collator:  torch.Size([3, 312, 477])\n",
      "img.shape in collator:  torch.Size([3, 154, 416])\n",
      "img.shape in collator:  torch.Size([3, 69, 134])\n",
      "img.shape in collator:  torch.Size([3, 94, 280])\n",
      "img.shape in collator:  torch.Size([3, 97, 207])\n",
      "img.shape in collator:  torch.Size([3, 491, 2126])\n",
      "img.shape in collator:  torch.Size([3, 147, 690])\n",
      "img.shape in collator:  torch.Size([3, 97, 459])\n",
      "img.shape in collator:  torch.Size([3, 97, 276])\n",
      "img.shape in collator:  torch.Size([3, 67, 291])\n",
      "img.shape in collator:  torch.Size([3, 74, 133])\n",
      "Conv Size:Conv Size:Conv Size: torch.Size([3, 512, 29, 532])\n",
      " torch.Size([3, 512, 29, 532])\n",
      "Conv Size: torch.Size([3, 512, 29, 532])\n",
      " torch.Size([3, 512, 29, 532])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 532])\n",
      "Resized/Cropped Conv Size:Resized/Cropped Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 1, 532])\n",
      "  torch.Size([3, 512, 1, 532])\n",
      "torch.Size([3, 512, 1, 532])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [2]/[4] Training:  33%|███▎      | 4/12 [00:01<00:01,  4.10it/s, loss=3.45, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape in collator:  torch.Size([3, 109, 199])\n",
      "img.shape in collator:  torch.Size([3, 114, 144])\n",
      "img.shape in collator:  torch.Size([3, 60, 159])\n",
      "img.shape in collator:  torch.Size([3, 45, 128])\n",
      "img.shape in collator:  torch.Size([3, 80, 268])\n",
      "img.shape in collator:  torch.Size([3, 43, 149])\n",
      "img.shape in collator:  torch.Size([3, 62, 201])\n",
      "img.shape in collator:  torch.Size([3, 199, 408])\n",
      "img.shape in collator:  torch.Size([3, 56, 144])\n",
      "img.shape in collator:  torch.Size([3, 49, 180])\n",
      "img.shape in collator:  torch.Size([3, 63, 223])\n",
      "img.shape in collator:  torch.Size([3, 62, 249])\n",
      "Conv Size: torch.Size([3, 512, 11, 103])\n",
      "Conv Size:Conv Size: torch.Size([3, 512, 11, 103])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 103])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 103])\n",
      "Conv Size: torch.Size([3, 512, 11, 103])\n",
      " torch.Size([3, 512, 11, 103])\n",
      "Resized/Cropped Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 1, 103])\n",
      " torch.Size([3, 512, 1, 103])\n",
      "img.shape in collator:  torch.Size([3, 301, 528])\n",
      "img.shape in collator:  torch.Size([3, 152, 269])\n",
      "img.shape in collator:  torch.Size([3, 83, 209])\n",
      "img.shape in collator:  torch.Size([3, 52, 139])\n",
      "img.shape in collator:  torch.Size([3, 52, 159])\n",
      "img.shape in collator:  torch.Size([3, 69, 212])\n",
      "img.shape in collator:  torch.Size([3, 104, 370])\n",
      "img.shape in collator:  torch.Size([3, 107, 310])\n",
      "img.shape in collator:  torch.Size([3, 89, 210])\n",
      "img.shape in collator:  torch.Size([3, 74, 173])\n",
      "img.shape in collator:  torch.Size([3, 50, 118])\n",
      "img.shape in collator:  torch.Size([3, 154, 406])\n",
      "Conv Size:Conv Size: torch.Size([3, 512, 17, 133])\n",
      "Resized/Cropped Conv Size:Conv Size: torch.Size([3, 512, 17, 133])\n",
      " torch.Size([3, 512, 17, 133])\n",
      " torch.Size([3, 512, 1, 133])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 133])\n",
      "Resized/Cropped Conv Size: Conv Size:torch.Size([3, 512, 1, 133])\n",
      " torch.Size([3, 512, 17, 133])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 133])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [2]/[4] Training:  42%|████▏     | 5/12 [00:01<00:01,  4.54it/s, loss=3.61, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape in collator:  torch.Size([3, 64, 183])\n",
      "img.shape in collator:  torch.Size([3, 54, 148])\n",
      "img.shape in collator:  torch.Size([3, 53, 221])\n",
      "img.shape in collator:  torch.Size([3, 93, 402])\n",
      "img.shape in collator:  torch.Size([3, 62, 168])\n",
      "img.shape in collator:  torch.Size([3, 91, 295])\n",
      "img.shape in collator:  torch.Size([3, 56, 143])\n",
      "img.shape in collator:  torch.Size([3, 66, 161])\n",
      "img.shape in collator:  torch.Size([3, 66, 176])\n",
      "img.shape in collator:  torch.Size([3, 509, 1158])\n",
      "img.shape in collator:  torch.Size([3, 74, 205])\n",
      "img.shape in collator:  torch.Size([3, 64, 127])\n",
      "Conv Size: torch.Size([3, 512, 30, 290])\n",
      "Conv Size: torch.Size([3, 512, 30, 290])\n",
      "Resized/Cropped Conv Size:Conv Size: torch.Size([3, 512, 30, 290])\n",
      " Conv Size: torch.Size([3, 512, 30, 290])\n",
      "Resized/Cropped Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 1, 290])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 290])\n",
      " torch.Size([3, 512, 1, 290])\n",
      "torch.Size([3, 512, 1, 290])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [2]/[4] Training:  50%|█████     | 6/12 [00:01<00:01,  3.69it/s, loss=4.07, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape in collator:  torch.Size([3, 52, 210])\n",
      "img.shape in collator:  torch.Size([3, 334, 657])\n",
      "img.shape in collator:  torch.Size([3, 39, 118])\n",
      "img.shape in collator:  torch.Size([3, 75, 300])\n",
      "img.shape in collator:  torch.Size([3, 78, 196])\n",
      "img.shape in collator:  torch.Size([3, 59, 230])\n",
      "img.shape in collator:  torch.Size([3, 52, 168])\n",
      "img.shape in collator:  torch.Size([3, 46, 105])\n",
      "img.shape in collator:  torch.Size([3, 35, 110])\n",
      "img.shape in collator:  torch.Size([3, 62, 181])\n",
      "img.shape in collator:  torch.Size([3, 56, 126])\n",
      "img.shape in collator:  torch.Size([3, 388, 1301])\n",
      "Conv Size:Conv Size: torch.Size([3, 512, 23, 326])\n",
      "Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 1, 326])\n",
      "Conv Size: torch.Size([3, 512, 23, 326])\n",
      " torch.Size([3, 512, 23, 326])Resized/Cropped Conv Size: torch.Size([3, 512, 1, 326])\n",
      "\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 23, 326])\n",
      " torch.Size([3, 512, 1, 326])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 326])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [2]/[4] Training:  67%|██████▋   | 8/12 [00:02<00:00,  4.14it/s, loss=3.64, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape in collator:  torch.Size([3, 76, 169])\n",
      "img.shape in collator:  torch.Size([3, 58, 176])\n",
      "img.shape in collator:  torch.Size([3, 93, 328])\n",
      "img.shape in collator:  torch.Size([3, 110, 153])\n",
      "img.shape in collator:  torch.Size([3, 86, 291])\n",
      "img.shape in collator:  torch.Size([3, 59, 80])\n",
      "img.shape in collator:  torch.Size([3, 57, 188])\n",
      "img.shape in collator:  torch.Size([3, 49, 162])\n",
      "img.shape in collator:  torch.Size([3, 51, 111])\n",
      "img.shape in collator:  torch.Size([3, 36, 72])\n",
      "img.shape in collator:  torch.Size([3, 75, 281])\n",
      "img.shape in collator:  torch.Size([3, 57, 177])\n",
      "Conv Size:Conv Size: torch.Size([3, 512, 5, 83])\n",
      " torch.Size([3, 512, 5, 83])\n",
      "Resized/Cropped Conv Size:Resized/Cropped Conv Size:Conv Size: torch.Size([3, 512, 5, 83])\n",
      " torch.Size([3, 512, 1, 83])\n",
      "Resized/Cropped Conv Size:Conv Size: torch.Size([3, 512, 5, 83])\n",
      "  torch.Size([3, 512, 1, 83])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 83])\n",
      "torch.Size([3, 512, 1, 83])\n",
      "img.shape in collator:  torch.Size([3, 68, 216])\n",
      "img.shape in collator:  torch.Size([3, 89, 313])\n",
      "img.shape in collator:  torch.Size([3, 74, 161])\n",
      "img.shape in collator:  torch.Size([3, 73, 283])\n",
      "img.shape in collator:  torch.Size([3, 62, 255])\n",
      "img.shape in collator:  torch.Size([3, 77, 157])\n",
      "img.shape in collator:  torch.Size([3, 526, 2043])\n",
      "img.shape in collator:  torch.Size([3, 236, 489])\n",
      "img.shape in collator:  torch.Size([3, 132, 373])\n",
      "img.shape in collator:  torch.Size([3, 127, 479])\n",
      "img.shape in collator:  torch.Size([3, 52, 168])\n",
      "img.shape in collator:  torch.Size([3, 74, 217])\n",
      "Conv Size:Conv Size: torch.Size([3, 512, 31, 511])\n",
      "Resized/Cropped Conv Size:Conv Size: torch.Size([3, 512, 31, 511])\n",
      " torch.Size([3, 512, 1, 511])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 511])\n",
      " torch.Size([3, 512, 31, 511])Conv Size:\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 511])\n",
      " torch.Size([3, 512, 31, 511])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 511])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [2]/[4] Training:  75%|███████▌  | 9/12 [00:02<00:01,  2.70it/s, loss=4.48, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape in collator:  torch.Size([3, 48, 142])\n",
      "img.shape in collator:  torch.Size([3, 239, 813])\n",
      "img.shape in collator:  torch.Size([3, 52, 145])\n",
      "img.shape in collator:  torch.Size([3, 47, 139])\n",
      "img.shape in collator:  torch.Size([3, 39, 179])\n",
      "img.shape in collator:  torch.Size([3, 115, 209])\n",
      "img.shape in collator:  torch.Size([3, 47, 124])\n",
      "img.shape in collator:  torch.Size([3, 71, 183])\n",
      "img.shape in collator:  torch.Size([3, 294, 1374])\n",
      "img.shape in collator:  torch.Size([3, 37, 189])\n",
      "img.shape in collator:  torch.Size([3, 72, 198])\n",
      "img.shape in collator:  torch.Size([3, 75, 215])\n",
      "Conv Size: Conv Size: torch.Size([3, 512, 17, 344])\n",
      "torch.Size([3, 512, 17, 344])\n",
      "Resized/Cropped Conv Size: Resized/Cropped Conv Size: torch.Size([3, 512, 1, 344])\n",
      "Conv Size:torch.Size([3, 512, 1, 344])\n",
      "Conv Size: torch.Size([3, 512, 17, 344])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 17, 344])\n",
      " torch.Size([3, 512, 1, 344])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 344])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [2]/[4] Training: 100%|██████████| 12/12 [00:03<00:00,  3.67it/s, loss=4.88, train_ca=nan, train_wa=nan]\n",
      "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape in collator:  torch.Size([3, 98, 206])\n",
      "img.shape in collator:  torch.Size([3, 56, 155])\n",
      "img.shape in collator:  torch.Size([3, 129, 312])\n",
      "img.shape in collator:  torch.Size([3, 61, 97])\n",
      "img.shape in collator:  torch.Size([3, 60, 110])\n",
      "img.shape in collator:  torch.Size([3, 163, 464])\n",
      "img.shape in collator:  torch.Size([3, 66, 205])\n",
      "img.shape in collator:  torch.Size([3, 48, 67])\n",
      "img.shape in collator:  torch.Size([3, 91, 214])\n",
      "img.shape in collator:  torch.Size([3, 110, 399])\n",
      "img.shape in collator:  torch.Size([3, 59, 194])\n",
      "img.shape in collator:  torch.Size([3, 49, 120])\n",
      "Conv Size: torch.Size([3, 512, 9, 117])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 117])\n",
      "Conv Size:Conv Size: torch.Size([3, 512, 9, 117])\n",
      " torch.Size([3, 512, 9, 117])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 117])\n",
      "Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 9, 117])\n",
      " Resized/Cropped Conv Size: torch.Size([3, 512, 1, 117])\n",
      "torch.Size([3, 512, 1, 117])\n",
      "img.shape in collator:  torch.Size([3, 43, 74])\n",
      "Conv Size: torch.Size([1, 512, 1, 19])\n",
      "img.shape in collator:  torch.Size([3, 167, 247])\n",
      "img.shape in collator:  torch.Size([3, 293, 684])\n",
      "img.shape in collator:  torch.Size([3, 44, 110])\n",
      "img.shape in collator:  torch.Size([3, 47, 144])\n",
      "img.shape in collator:  torch.Size([3, 59, 177])\n",
      "img.shape in collator:  torch.Size([3, 114, 550])\n",
      "img.shape in collator:  torch.Size([3, 108, 351])\n",
      "img.shape in collator:  torch.Size([3, 68, 164])\n",
      "img.shape in collator:  torch.Size([3, 110, 203])\n",
      "img.shape in collator:  torch.Size([3, 33, 97])\n",
      "img.shape in collator:  torch.Size([3, 68, 209])\n",
      "img.shape in collator:  torch.Size([3, 87, 243])\n",
      "Conv Size: torch.Size([3, 512, 17, 172])\n",
      "Resized/Cropped Conv Size:Conv Size: torch.Size([3, 512, 1, 172])\n",
      " torch.Size([3, 512, 17, 172])\n",
      "Resized/Cropped Conv Size:Conv Size:  torch.Size([3, 512, 1, 172])\n",
      "Conv Size: torch.Size([3, 512, 17, 172])\n",
      "torch.Size([3, 512, 17, 172])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 172])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  67%|██████▋   | 2/3 [00:00<00:00,  7.28it/s, val_loss=3.61, val_ca=0, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape in collator:  torch.Size([3, 39, 153])\n",
      "img.shape in collator:  torch.Size([3, 77, 160])\n",
      "img.shape in collator:  torch.Size([3, 239, 459])\n",
      "img.shape in collator:  torch.Size([3, 108, 243])\n",
      "img.shape in collator:  torch.Size([3, 638, 1190])\n",
      "img.shape in collator:  torch.Size([3, 143, 230])\n",
      "img.shape in collator:  torch.Size([3, 84, 289])\n",
      "img.shape in collator:  torch.Size([3, 86, 204])\n",
      "img.shape in collator:  torch.Size([3, 71, 225])\n",
      "img.shape in collator:  torch.Size([3, 84, 188])\n",
      "img.shape in collator:  torch.Size([3, 65, 71])\n",
      "img.shape in collator:  torch.Size([3, 68, 155])\n",
      "Conv Size:Conv Size: torch.Size([3, 512, 38, 298])\n",
      "Conv Size: torch.Size([3, 512, 38, 298])\n",
      " torch.Size([3, 512, 38, 298])Resized/Cropped Conv Size: torch.Size([3, 512, 1, 298])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 298])\n",
      "\n",
      "Resized/Cropped Conv Size:Conv Size: torch.Size([3, 512, 1, 298])\n",
      " torch.Size([3, 512, 38, 298])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 298])\n",
      "img.shape in collator:  torch.Size([3, 99, 296])\n",
      "img.shape in collator:  torch.Size([3, 122, 274])\n",
      "img.shape in collator:  torch.Size([3, 203, 480])\n",
      "img.shape in collator:  torch.Size([3, 47, 150])\n",
      "img.shape in collator:  torch.Size([3, 56, 216])\n",
      "img.shape in collator:  torch.Size([3, 58, 119])\n",
      "img.shape in collator:  torch.Size([3, 61, 260])\n",
      "img.shape in collator:  torch.Size([3, 103, 485])\n",
      "img.shape in collator:  torch.Size([3, 71, 248])\n",
      "img.shape in collator:  torch.Size([3, 81, 222])\n",
      "Conv Size: torch.Size([3, 512, 11, 122])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 3/3 [00:00<00:00,  7.49it/s, val_loss=3.68, val_ca=0, val_wa=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv Size: torch.Size([3, 512, 11, 122])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 122])\n",
      "Conv Size:Resized/Cropped Conv Size: torch.Size([3, 512, 1, 122])\n",
      " torch.Size([3, 512, 11, 122])\n",
      "Resized/Cropped Conv Size: torch.Size([3, 512, 1, 122])\n",
      "Conv Size: torch.Size([1, 512, 11, 122])\n",
      "Resized/Cropped Conv Size: torch.Size([1, 512, 1, 122])\n",
      "3.7484677036603293\n",
      "Validation loss decreased (3.811955 --> 3.748468).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "alphabet = \"\"\"Only thewigsofrcvdampbkuq.$A-210xT5'MDL,RYHJ\"ISPWENj&BC93VGFKz();#:!7U64Q8?+*ZX/%\"\"\"\n",
    "args = {\n",
    "    'name':'exp1',\n",
    "    'path':'../data/LP-characters',\n",
    "    'imgdir': 'train',\n",
    "    'imgH':48,\n",
    "    'nChannels':3,\n",
    "    'nHidden':256,\n",
    "    'nClasses':len(alphabet),\n",
    "    'lr':0.001,\n",
    "    'epochs':4,\n",
    "    'batch_size':12,\n",
    "    'save_dir':'../checkpoints/',\n",
    "    'log_dir':'../logs',\n",
    "    'resume':False,\n",
    "    'cuda':True,\n",
    "    'schedule':False\n",
    "    \n",
    "}\n",
    "\n",
    "data = PlateDataset(args)\n",
    "args['collate_fn'] = PlateCollator()\n",
    "train_split = int(0.8*len(data))\n",
    "val_split = len(data) - train_split\n",
    "args['data_train'], args['data_val'] = random_split(data, (train_split, val_split))\n",
    "print('Traininig Data Size:{}\\nVal Data Size:{}'.format(\n",
    "    len(args['data_train']), len(args['data_val'])))\n",
    "args['alphabet'] = alphabet\n",
    "model = CRNN(args)\n",
    "args['criterion'] = CustomCTCLoss()\n",
    "savepath = os.path.join(args['save_dir'], args['name'])\n",
    "gmkdir(savepath)\n",
    "gmkdir(args['log_dir'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
    "learner = Learner(model, optimizer, savepath=savepath, resume=args['resume'])\n",
    "learner.fit(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.is_available() True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"torch.cuda.is_available()\", torch.cuda.is_available())\n",
    "\n",
    "def get_accuracy(args):\n",
    "    loader = torch.utils.data.DataLoader(args['data'],\n",
    "                                         batch_size=args['batch_size'],\n",
    "                                         collate_fn=args['collate_fn'])\n",
    "    model = args['model']\n",
    "    model.eval()\n",
    "    converter = OCRLabelConverter(args['alphabet'])\n",
    "    evaluator = Eval()\n",
    "    labels, predictions, images = [], [], []\n",
    "    \n",
    "    target_size = (79, 237) \n",
    "    \n",
    "    for iteration, batch in enumerate(tqdm(loader)):\n",
    "        input_, targets = batch['img'].to(device), batch['label']\n",
    "        print(\"targets: \", targets)\n",
    "        images.extend(input_.squeeze().detach())\n",
    "        labels.extend(targets)\n",
    "        \n",
    "        # Resize or pad images to the target size\n",
    "        resized_images = []\n",
    "        for img in input_:\n",
    "            # Move to CPU\n",
    "            img_cpu = img.cpu()\n",
    "\n",
    "            # Convert PyTorch Tensor to PIL Image\n",
    "            img_pil = transforms.ToPILImage()(img_cpu)\n",
    "            \n",
    "            # Resize PIL Image\n",
    "            #img_resized = transforms.functional.resize(img_pil, target_size)\n",
    "            \n",
    "            # If you want to pad instead of resize, use the following line\n",
    "            #img_resized = transforms.functional.pad(img_pil, padding)\n",
    "            \n",
    "            # Convert PIL Image back to PyTorch Tensor\n",
    "            img_resized = transforms.ToTensor()(img_resized)\n",
    "            \n",
    "            resized_images.append(img_resized)\n",
    "\n",
    "        # Stack resized or padded images into a single tensor\n",
    "        #input_resized = torch.stack(resized_images)\n",
    "        input_resized = torch.stack(resized_images).to(device) \n",
    "        \n",
    "        targets, lengths = converter.encode(targets)\n",
    "        \n",
    "        logits = model(input_).transpose(1, 0)\n",
    "        logits = torch.nn.functional.log_softmax(logits, 2)\n",
    "        logits = logits.contiguous().cpu()\n",
    "        \n",
    "        # Debugging prints\n",
    "        print(\"Logits shape:\", logits.shape)\n",
    "        print(\"Targets:\", targets)\n",
    "        \n",
    "        T, B, H = logits.size()\n",
    "        pred_sizes = torch.LongTensor([T for i in range(B)])\n",
    "        probs, pos = logits.max(2)\n",
    "        pos = pos.transpose(1, 0).contiguous().view(-1)\n",
    "        sim_preds = converter.decode(pos.data, pred_sizes.data, raw=False)\n",
    "        \n",
    "        # Debugging print\n",
    "        print(\"Ground Truth Labels:\", targets)\n",
    "        print(\"Decoded Labels (sim_preds):\", sim_preds)\n",
    "        predictions.extend(sim_preds)\n",
    "        \n",
    "        # Debugging print for decoded labels\n",
    "        decoded_labels = converter.decode(targets.data, lengths.data, raw=False)\n",
    "        print(\"Decoded Labels:\", decoded_labels)\n",
    "        \n",
    "        predictions.extend(sim_preds)\n",
    "\n",
    "        # Debugging prints\n",
    "        print(\"Predictions (sim_preds):\", sim_preds)\n",
    "    # Visualize the images\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    columns = 4\n",
    "    rows = 5\n",
    "    pairs = list(zip(images, predictions))\n",
    "    indices = np.random.permutation(len(pairs))\n",
    "    for i in range(1, columns * rows + 1):\n",
    "        img = images[indices[i]].cpu().numpy()\n",
    "        img = np.transpose(img, (1, 2, 0))  # Change the order of dimensions\n",
    "        img = (img - img.min()) / (img.max() - img.min())\n",
    "        img = np.array(img * 255.0, dtype=np.uint8)\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        plt.title(predictions[indices[i]])\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img)\n",
    "    plt.show()\n",
    "        \n",
    "    ca = np.mean((list(map(evaluator.char_accuracy, list(zip(predictions, labels))))))\n",
    "    wa = np.mean((list(map(evaluator.word_accuracy_line, list(zip(predictions, labels))))))\n",
    "    return ca, wa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model ../checkpoints/exp1/best.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:00<00:00,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets:  ['KA031351', 'TN74F3339', 'MH03BS7778', 'KL02AF6363', 'KA21M5519', 'KL16J3636', 'MH06A8929W', 'MH20EE7601', 'KA51AA3469', 'MH04DW8351', 'KL06H5834', 'GJ01MW7581']\n",
      "Conv Size: torch.Size([12, 512, 9, 89])\n",
      "Resized/Cropped Conv Size: torch.Size([12, 512, 1, 89])\n",
      "Logits shape: torch.Size([89, 12, 81])\n",
      "Targets: tensor([61, 28, 32, 57, 31, 57, 35, 31, 34, 51, 69, 72, 60, 57, 57, 57, 56, 37,\n",
      "        43, 32, 57, 54, 47, 69, 69, 69, 74, 61, 39, 32, 30, 28, 60, 71, 57, 71,\n",
      "        57, 61, 28, 30, 31, 37, 35, 35, 31, 56, 61, 39, 31, 71, 44, 57, 71, 57,\n",
      "        71, 37, 43, 32, 71, 28, 74, 56, 30, 56, 49, 37, 43, 30, 32, 50, 50, 69,\n",
      "        71, 32, 31, 61, 28, 35, 31, 28, 28, 57, 72, 71, 56, 37, 43, 32, 72, 38,\n",
      "        49, 74, 57, 35, 31, 61, 39, 32, 71, 43, 35, 74, 57, 72, 59, 44, 32, 31,\n",
      "        37, 49, 69, 35, 74, 31], dtype=torch.int32)\n",
      "Ground Truth Labels: tensor([61, 28, 32, 57, 31, 57, 35, 31, 34, 51, 69, 72, 60, 57, 57, 57, 56, 37,\n",
      "        43, 32, 57, 54, 47, 69, 69, 69, 74, 61, 39, 32, 30, 28, 60, 71, 57, 71,\n",
      "        57, 61, 28, 30, 31, 37, 35, 35, 31, 56, 61, 39, 31, 71, 44, 57, 71, 57,\n",
      "        71, 37, 43, 32, 71, 28, 74, 56, 30, 56, 49, 37, 43, 30, 32, 50, 50, 69,\n",
      "        71, 32, 31, 61, 28, 35, 31, 28, 28, 57, 72, 71, 56, 37, 43, 32, 72, 38,\n",
      "        49, 74, 57, 35, 31, 61, 39, 32, 71, 43, 35, 74, 57, 72, 59, 44, 32, 31,\n",
      "        37, 49, 69, 35, 74, 31], dtype=torch.int32)\n",
      "Decoded Labels (sim_preds): ['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "Decoded Labels: ['KA031351', 'TN74F39', 'MH03BS78', 'KL02AF6363', 'KA21M519', 'KL16J3636', 'MH06A8929W', 'MH20E7601', 'KA51A3469', 'MH04DW8351', 'KL06H5834', 'GJ01MW7581']\n",
      "Predictions (sim_preds): ['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "targets:  ['TN52U1580', 'CL31VLGP', 'KL10AV6633', 'MH20B4Y546', 'DL7SBS6930', 'AK5600MH12', 'KL09AL9540', 'MH02CT2727', 'KL43B2344', 'AP02BP2454', 'TN59AQ1515', 'HR26CT4063']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 2/4 [00:00<00:00,  5.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv Size: torch.Size([12, 512, 13, 104])\n",
      "Resized/Cropped Conv Size: torch.Size([12, 512, 1, 104])\n",
      "Logits shape: torch.Size([104, 12, 81])\n",
      "Targets: tensor([34, 51, 35, 30, 70, 31, 35, 74, 32, 55, 39, 57, 31, 58, 39, 59, 48, 61,\n",
      "        39, 31, 32, 28, 58, 71, 71, 57, 57, 37, 43, 30, 32, 54, 72, 42, 35, 72,\n",
      "        71, 38, 39, 69, 47, 54, 47, 71, 56, 57, 32, 28, 61, 35, 71, 32, 32, 37,\n",
      "        43, 31, 30, 61, 39, 32, 56, 28, 39, 56, 35, 72, 32, 37, 43, 32, 30, 55,\n",
      "        34, 30, 69, 30, 69, 61, 39, 72, 57, 54, 30, 57, 72, 72, 28, 48, 32, 30,\n",
      "        54, 48, 30, 72, 35, 72, 34, 51, 35, 56, 28, 73, 31, 35, 31, 35, 43, 41,\n",
      "        30, 71, 55, 34, 72, 32, 71, 57], dtype=torch.int32)\n",
      "Ground Truth Labels: tensor([34, 51, 35, 30, 70, 31, 35, 74, 32, 55, 39, 57, 31, 58, 39, 59, 48, 61,\n",
      "        39, 31, 32, 28, 58, 71, 71, 57, 57, 37, 43, 30, 32, 54, 72, 42, 35, 72,\n",
      "        71, 38, 39, 69, 47, 54, 47, 71, 56, 57, 32, 28, 61, 35, 71, 32, 32, 37,\n",
      "        43, 31, 30, 61, 39, 32, 56, 28, 39, 56, 35, 72, 32, 37, 43, 32, 30, 55,\n",
      "        34, 30, 69, 30, 69, 61, 39, 72, 57, 54, 30, 57, 72, 72, 28, 48, 32, 30,\n",
      "        54, 48, 30, 72, 35, 72, 34, 51, 35, 56, 28, 73, 31, 35, 31, 35, 43, 41,\n",
      "        30, 71, 55, 34, 72, 32, 71, 57], dtype=torch.int32)\n",
      "Decoded Labels (sim_preds): ['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "Decoded Labels: ['TN52U1580', 'CL31VLGP', 'KL10AV63', 'MH20B4Y546', 'DL7SBS6930', 'AK560MH12', 'KL09AL9540', 'MH02CT2727', 'KL43B234', 'AP02BP2454', 'TN59AQ1515', 'HR26CT4063']\n",
      "Predictions (sim_preds): ['', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 3/4 [00:00<00:00,  4.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets:  ['KL10AW2111', 'HR26CM6005', 'KL10AV6342', '2HR6U7501', 'KL49H5270', 'HR26DK0830', 'KA01D0133', 'MH2OBN3525', 'MH20CS4946', 'TN45BA1065', 'KL53E964', 'MH14TCD204']\n",
      "Conv Size: torch.Size([12, 512, 13, 141])\n",
      "Resized/Cropped Conv Size: torch.Size([12, 512, 1, 141])\n",
      "Logits shape: torch.Size([141, 12, 81])\n",
      "Targets: tensor([61, 39, 31, 32, 28, 49, 30, 31, 31, 31, 43, 41, 30, 71, 55, 37, 71, 32,\n",
      "        32, 35, 61, 39, 31, 32, 28, 58, 71, 57, 72, 30, 30, 43, 41, 71, 70, 69,\n",
      "        35, 32, 31, 61, 39, 72, 56, 43, 35, 30, 69, 32, 43, 41, 30, 71, 38, 61,\n",
      "        32, 74, 57, 32, 61, 28, 32, 31, 38, 32, 31, 57, 57, 37, 43, 30,  1, 54,\n",
      "        51, 57, 35, 30, 35, 37, 43, 30, 32, 55, 47, 72, 56, 72, 71, 34, 51, 72,\n",
      "        35, 54, 28, 31, 32, 71, 35, 61, 39, 35, 57, 50, 56, 71, 72, 37, 43, 31,\n",
      "        72, 34, 55, 38, 30, 32, 72], dtype=torch.int32)\n",
      "Ground Truth Labels: tensor([61, 39, 31, 32, 28, 49, 30, 31, 31, 31, 43, 41, 30, 71, 55, 37, 71, 32,\n",
      "        32, 35, 61, 39, 31, 32, 28, 58, 71, 57, 72, 30, 30, 43, 41, 71, 70, 69,\n",
      "        35, 32, 31, 61, 39, 72, 56, 43, 35, 30, 69, 32, 43, 41, 30, 71, 38, 61,\n",
      "        32, 74, 57, 32, 61, 28, 32, 31, 38, 32, 31, 57, 57, 37, 43, 30,  1, 54,\n",
      "        51, 57, 35, 30, 35, 37, 43, 30, 32, 55, 47, 72, 56, 72, 71, 34, 51, 72,\n",
      "        35, 54, 28, 31, 32, 71, 35, 61, 39, 35, 57, 50, 56, 71, 72, 37, 43, 31,\n",
      "        72, 34, 55, 38, 30, 32, 72], dtype=torch.int32)\n",
      "Decoded Labels (sim_preds): ['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "Decoded Labels: ['KL10AW21', 'HR26CM605', 'KL10AV6342', '2HR6U7501', 'KL49H5270', 'HR26DK0830', 'KA01D013', 'MH2OBN3525', 'MH20CS4946', 'TN45BA1065', 'KL53E964', 'MH14TCD204']\n",
      "Predictions (sim_preds): ['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "targets:  ['TN07B5U472', 'MH20EE045', 'TN74A5L074', 'KA19TR0220102011', 'MH20CS1941', 'H20CS1938']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00,  5.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv Size: torch.Size([6, 512, 4, 54])\n",
      "Resized/Cropped Conv Size: torch.Size([6, 512, 1, 54])\n",
      "Logits shape: torch.Size([54, 6, 81])\n",
      "Targets: tensor([34, 51, 32, 69, 54, 35, 70, 72, 69, 30, 37, 43, 30, 32, 50, 50, 32, 72,\n",
      "        35, 34, 51, 69, 72, 28, 35, 39, 32, 69, 72, 61, 28, 31, 56, 34, 41, 32,\n",
      "        30, 30, 32, 31, 32, 30, 32, 31, 31, 37, 43, 30, 32, 55, 47, 31, 56, 72,\n",
      "        31, 43, 30, 32, 55, 47, 31, 56, 57, 74], dtype=torch.int32)\n",
      "Ground Truth Labels: tensor([34, 51, 32, 69, 54, 35, 70, 72, 69, 30, 37, 43, 30, 32, 50, 50, 32, 72,\n",
      "        35, 34, 51, 69, 72, 28, 35, 39, 32, 69, 72, 61, 28, 31, 56, 34, 41, 32,\n",
      "        30, 30, 32, 31, 32, 30, 32, 31, 31, 37, 43, 30, 32, 55, 47, 31, 56, 72,\n",
      "        31, 43, 30, 32, 55, 47, 31, 56, 57, 74], dtype=torch.int32)\n",
      "Decoded Labels (sim_preds): ['', '', '', '', '', '']\n",
      "Decoded Labels: ['TN07B5U472', 'MH20E045', 'TN74A5L074', 'KA19TR02010201', 'MH20CS1941', 'H20CS1938']\n",
      "Predictions (sim_preds): ['', '', '', '', '', '']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Accuracy: 0.00\n",
      "Word Accuracy: nan\n"
     ]
    }
   ],
   "source": [
    "args['imgdir'] = 'test'\n",
    "args['data'] = PlateDataset(args)\n",
    "resume_file = os.path.join(args['save_dir'], args['name'], 'best.ckpt')\n",
    "if os.path.isfile(resume_file):\n",
    "    print('Loading model %s'%resume_file)\n",
    "    checkpoint = torch.load(resume_file)\n",
    "    model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "    args['model'] = model\n",
    "    ca, wa = get_accuracy(args)\n",
    "    print(\"Character Accuracy: %.2f\\nWord Accuracy: %.2f\"%(ca, wa))\n",
    "else:\n",
    "    print(\"=> no checkpoint found at '{}'\".format(os.path.join(args['save_dir'], args['name'])))\n",
    "    print('Exiting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "a1_env",
   "language": "python",
   "name": "a1_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
